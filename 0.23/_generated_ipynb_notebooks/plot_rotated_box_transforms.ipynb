{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Transforms on Rotated Bounding Boxes\n\nThis example illustrates how to define and use rotated bounding boxes.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Support for rotated bounding boxes was released in TorchVision 0.23 and is\n    currently a BETA feature. We don't expect the API to change, but there may\n    be some rare edge-cases. If you find any issues, please report them on\n    our bug tracker: https://github.com/pytorch/vision/issues?q=is:open+is:issue</p></div>\n\nFirst, a bit of setup code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torchvision.tv_tensors import BoundingBoxes\nfrom torchvision.transforms import v2\nfrom helpers import plot\n\nplt.rcParams[\"figure.figsize\"] = [10, 5]\nplt.rcParams[\"savefig.bbox\"] = \"tight\"\n\n# if you change the seed, make sure that the randomly-applied transforms\n# properly show that the image can be both transformed and *not* transformed!\ntorch.manual_seed(0)\n\n# If you're trying to run that on Colab, you can download the assets and the\n# helpers from https://github.com/pytorch/vision/tree/main/gallery/\norig_img = Image.open(Path('../assets') / 'leaning_tower.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Rotated Bounding Box\nRotated bounding boxes are created by instantiating the\n:class:`~torchvision.tv_tensors.BoundingBoxes` class. It's the ``format``\nparameter of the constructor that determines if a bounding box is rotated or\nnot. In this instance, we use the CXCYWHR\n:attr:`~torchvision.tv_tensors.BoundingBoxFormat`. The first two values are\nthe X and Y coordinates of the center of the bounding box.  The next two\nvalues are the width and height of the bounding box, and the last value is the\nrotation of the bounding box, in degrees.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "orig_box = BoundingBoxes(\n    [\n        [860.0, 1100, 570, 1840, -7],\n    ],\n    format=\"CXCYWHR\",\n    canvas_size=(orig_img.size[1], orig_img.size[0]),\n)\n\nplot([(orig_img, orig_box)], bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms illustrations\n\nUsing :class:`~torchvision.transforms.RandomRotation`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rotater = v2.RandomRotation(degrees=(0, 180), expand=True)\nrotated_imgs = [rotater((orig_img, orig_box)) for _ in range(4)]\nplot([(orig_img, orig_box)] + rotated_imgs, bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using :class:`~torchvision.transforms.Pad`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "padded_imgs_and_boxes = [\n    v2.Pad(padding=padding)(orig_img, orig_box)\n    for padding in (30, 50, 100, 200)\n]\nplot([(orig_img, orig_box)] + padded_imgs_and_boxes, bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using :class:`~torchvision.transforms.Resize`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resized_imgs = [\n    v2.Resize(size=size)(orig_img, orig_box)\n    for size in (30, 50, 100, orig_img.size)\n]\nplot([(orig_img, orig_box)] + resized_imgs, bbox_width=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the bounding box looking bigger in the images with less pixels is\nan artifact, not reality. That is merely the rasterised representation of the\nbounding box's boundaries appearing bigger because we specify a fixed width of\nthat rasterized line. When the image is, say, only 30 pixels wide, a\nline that is 3 pixels wide is relatively large.\n\n\n## Clamping Mode, and its effect on transforms\n\nSome transforms, such as :class:`~torchvision.transforms.CenterCrop`, may\nresult in having the transformed bounding box partially outside of the\ntransformed (cropped) image. In general, this may happen on most of the\n`geometric transforms <v2_api_ref>`.\n\nIn such cases, the bounding box is clamped to the transformed image size based\non its ``clamping_mode`` attribute.  There are three values for\n``clamping_mode``, which determines how the box is clamped after a\ntransformation:\n\n - ``None``: No clamping is applied, and the bounding box may be partially\n   outside of the image.\n - `\"hard\"`:  The box is clamped to the image size, such that all its corners\n   are within the image canvas. This potentially results in a loss of\n   information, and it can lead to unintuitive resuts. But may be necessary\n   for some applications e.g. if the model doesn't support boxes outside of\n   their image.\n - `\"soft\"`: . This is an intermediate mode between ``None`` and \"hard\": the\n   box is clamped, but not as strictly as in \"hard\" mode. Some box dimensions\n   may still be outside of the image. This is the default when constucting\n   :class:`~torchvision.tv_tensors.BoundingBoxes`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For axis-aligned bounding boxes, the `\"soft\"` and `\"hard\"` modes behave\n      the same, as the bounding box is always clamped to the image size.</p></div>\n\nLet's illustrate the clamping modes with\n:class:`~torchvision.transforms.CenterCrop` transform:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert orig_box.clamping_mode == \"soft\"\n\nbox_hard_clamping = BoundingBoxes(orig_box, format=orig_box.format, canvas_size=orig_box.canvas_size, clamping_mode=\"hard\")\n\nbox_no_clamping = BoundingBoxes(orig_box, format=orig_box.format, canvas_size=orig_box.canvas_size, clamping_mode=None)\n\ncrop_sizes = (800, 1200, 2000, orig_img.size)\nsoft_center_crops_and_boxes = [\n    v2.CenterCrop(size=size)(orig_img, orig_box)\n    for size in crop_sizes\n]\n\nhard_center_crops_and_boxes = [\n    v2.CenterCrop(size=size)(orig_img, box_hard_clamping)\n    for size in crop_sizes\n]\n\nno_clamping_center_crops_and_boxes = [\n    v2.CenterCrop(size=size)(orig_img, box_no_clamping)\n    for size in crop_sizes\n]\n\nplot([[(orig_img, box_hard_clamping)] + hard_center_crops_and_boxes,\n      [(orig_img, orig_box)] + soft_center_crops_and_boxes,\n      [(orig_img, box_no_clamping)] + no_clamping_center_crops_and_boxes],\n     bbox_width=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above shows the \"hard\" clamping mode, \"soft\" and ``None``, in this\norder. While \"soft\" and ``None`` result in similar plots, they do not lead to\nthe exact same clamped boxes. The non-clamped boxes will show dimensions that are further away from the image:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"boxes with soft clamping:\")\nprint(soft_center_crops_and_boxes)\nprint()\nprint(\"boxes with no clamping:\")\nprint(no_clamping_center_crops_and_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting the clamping mode\n\nThe ``clamping_mode`` attribute, which determines the clamping strategy that\nis applied to a box, can be set in different ways:\n\n- When constructing the bounding box with its\n  :class:`~torchvision.tv_tensors.BoundingBoxes` constructor, as done in the example above.\n- By directly setting the attribute on an existing instance, e.g. ``boxes.clamping_mode = \"hard\"``.\n- By calling the :class:`~torchvision.transforms.v2.SetClampingMode` transform.\n\nAlso, remember that you can always clamp the bounding box manually by\ncalling the :meth:`~torchvision.transforms.v2.ClampBoundingBoxes` transform!\nHere's an example illustrating all of these option:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t = v2.Compose([\n    v2.CenterCrop(size=(800,)),  # clamps according to the current clamping_mode\n                                 # attribute, in this case set by the constructor\n    v2.SetClampingMode(None),  # sets the clamping_mode attribute for future transforms\n    v2.Pad(padding=3),  # clamps according to the current clamping_mode\n                        # i.e. ``None``\n    v2.ClampBoundingBoxes(clamping_mode=\"soft\"),  # clamps with \"soft\" mode.\n])\n\nout_img, out_box = t(orig_img, orig_box)\nplot([(orig_img, orig_box), (out_img, out_box)], bbox_width=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}