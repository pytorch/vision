{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to write your own v2 transforms\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [Colab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_custom_transforms.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_transforms_plot_custom_transforms.py>` to download the full example code.</p></div>\n\nThis guide explains how to write transforms that are compatible with the\ntorchvision transforms V2 API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List\n\nimport torch\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Just create a ``nn.Module`` and override the ``forward`` method\n\nIn most cases, this is all you're going to need, as long as you already know\nthe structure of the input that your transform will expect. For example if\nyou're just doing image classification, your transform will typically accept a\nsingle image as input, or a ``(img, label)`` input. So you can just hard-code\nyour ``forward`` method to accept just that, e.g.\n\n.. code:: python\n\n    class MyCustomTransform(torch.nn.Module):\n        def forward(self, img, label):\n            # Do some transformations\n            return new_img, new_label\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This means that if you have a custom transform that is already compatible\n    with the V1 transforms (those in ``torchvision.transforms``), it will\n    still work with the V2 transforms without any change!</p></div>\n\nWe will illustrate this more completely below with a typical detection case,\nwhere our samples are just images, bounding boxes and labels:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyCustomTransform(torch.nn.Module):\n    def forward(self, img, bboxes, label):  # we assume inputs are always structured like this\n        print(\n            f\"I'm transforming an image of shape {img.shape} \"\n            f\"with bboxes = {bboxes}\\n{label = }\"\n        )\n        # Do some transformations. Here, we're just passing though the input\n        return img, bboxes, label\n\n\ntransforms = v2.Compose([\n    MyCustomTransform(),\n    v2.RandomResizedCrop((224, 224), antialias=True),\n    v2.RandomHorizontalFlip(p=1),\n    v2.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n])\n\nH, W = 256, 256\nimg = torch.rand(3, H, W)\nbboxes = tv_tensors.BoundingBoxes(\n    torch.tensor([[0, 10, 10, 20], [50, 50, 70, 70]]),\n    format=\"XYXY\",\n    canvas_size=(H, W)\n)\nlabel = 3\n\nout_img, out_bboxes, out_label = transforms(img, bboxes, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Output image shape: {out_img.shape}\\nout_bboxes = {out_bboxes}\\n{out_label = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>While working with TVTensor classes in your code, make sure to\n    familiarize yourself with this section:\n    `tv_tensor_unwrapping_behaviour`</p></div>\n\n## Supporting arbitrary input structures\n\nIn the section above, we have assumed that you already know the structure of\nyour inputs and that you're OK with hard-coding this expected structure in\nyour code. If you want your custom transforms to be as flexible as possible,\nthis can be a bit limiting.\n\nA key feature of the builtin Torchvision V2 transforms is that they can accept\narbitrary input structure and return the same structure as output (with\ntransformed entries). For example, transforms can accept a single image, or a\ntuple of ``(img, label)``, or an arbitrary nested dictionary as input. Here's\nan example on the built-in transform :class:`~torchvision.transforms.v2.RandomHorizontalFlip`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "structured_input = {\n    \"img\": img,\n    \"annotations\": (bboxes, label),\n    \"something that will be ignored\": (1, \"hello\"),\n    \"another tensor that is ignored\": torch.arange(10),\n}\nstructured_output = v2.RandomHorizontalFlip(p=1)(structured_input)\n\nassert isinstance(structured_output, dict)\nassert structured_output[\"something that will be ignored\"] == (1, \"hello\")\nassert (structured_output[\"another tensor that is ignored\"] == torch.arange(10)).all()\nprint(f\"The input bboxes are:\\n{structured_input['annotations'][0]}\")\nprint(f\"The transformed bboxes are:\\n{structured_output['annotations'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basics: override the `transform()` method\n\nIn order to support arbitrary inputs in your custom transform, you will need\nto inherit from :class:`~torchvision.transforms.v2.Transform` and override the\n`.transform()` method (not the `forward()` method!). Below is a basic example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyCustomTransform(v2.Transform):\n    def transform(self, inpt: Any, params: Dict[str, Any]):\n        if type(inpt) == torch.Tensor:\n            print(f\"I'm transforming an image of shape {inpt.shape}\")\n            return inpt + 1  # dummy transformation\n        elif isinstance(inpt, tv_tensors.BoundingBoxes):\n            print(f\"I'm transforming bounding boxes! {inpt.canvas_size = }\")\n            return tv_tensors.wrap(inpt + 100, like=inpt)  # dummy transformation\n\n\nmy_custom_transform = MyCustomTransform()\nstructured_output = my_custom_transform(structured_input)\n\nassert isinstance(structured_output, dict)\nassert structured_output[\"something that will be ignored\"] == (1, \"hello\")\nassert (structured_output[\"another tensor that is ignored\"] == torch.arange(10)).all()\nprint(f\"The input bboxes are:\\n{structured_input['annotations'][0]}\")\nprint(f\"The transformed bboxes are:\\n{structured_output['annotations'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An important thing to note is that when we call ``my_custom_transform`` on\n``structured_input``, the input is flattened and then each individual part is\npassed to ``transform()``. That is, ``transform()``` receives the input image,\nthen the bounding boxes, etc. Within ``transform()``, you can decide how to\ntransform each input, based on their type.\n\nIf you're curious why the other tensor (``torch.arange()``) didn't get passed\nto ``transform()``, see `this note <passthrough_heuristic>` for more\ndetails.\n\n### Advanced: The ``make_params()`` method\n\nThe ``make_params()`` method is called internally before calling\n``transform()`` on each input. This is typically useful to generate random\nparameter values. In the example below, we use it to randomly apply the\ntransformation with a probability of 0.5\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyRandomTransform(MyCustomTransform):\n    def __init__(self, p=0.5):\n        self.p = p\n        super().__init__()\n\n    def make_params(self, flat_inputs: List[Any]) -> Dict[str, Any]:\n        apply_transform = (torch.rand(size=(1,)) < self.p).item()\n        params = dict(apply_transform=apply_transform)\n        return params\n\n    def transform(self, inpt: Any, params: Dict[str, Any]):\n        if not params[\"apply_transform\"]:\n            print(\"Not transforming anything!\")\n            return inpt\n        else:\n            return super().transform(inpt, params)\n\n\nmy_random_transform = MyRandomTransform()\n\ntorch.manual_seed(0)\n_ = my_random_transform(structured_input)  # transforms\n_ = my_random_transform(structured_input)  # doesn't transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>It's important for such random parameter generation to happen within\n    ``make_params()`` and not within ``transform()``, so that for a given\n    transform call, the same RNG applies to all the inputs in the same way. If\n    we were to perform the RNG within ``transform()``, we would risk e.g.\n    transforming the image while *not* transforming the bounding boxes.</p></div>\n\nThe ``make_params()`` method takes the list of all the inputs as parameter\n(each of the elements in this list will later be pased to ``transform()``).\nYou can use ``flat_inputs`` to e.g. figure out the dimensions on the input,\nusing :func:`~torchvision.transforms.v2.query_chw` or\n:func:`~torchvision.transforms.v2.query_size`.\n\n``make_params()`` should return a dict (or actually, anything you want) that\nwill then be passed to ``transform()``.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}