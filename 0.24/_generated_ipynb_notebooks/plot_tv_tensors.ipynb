{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# TVTensors FAQ\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [Colab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_tv_tensors.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_transforms_plot_tv_tensors.py>` to download the full example code.</p></div>\n\n\nTVTensors are Tensor subclasses introduced together with\n``torchvision.transforms.v2``. This example showcases what these TVTensors are\nand how they behave.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>**Intended Audience** Unless you're writing your own transforms or your own TVTensors, you\n    probably do not need to read this guide. This is a fairly low-level topic\n    that most users will not need to worry about: you do not need to understand\n    the internals of TVTensors to efficiently rely on\n    ``torchvision.transforms.v2``. It may however be useful for advanced users\n    trying to implement their own datasets, transforms, or work directly with\n    the TVTensors.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n\nimport torch\nfrom torchvision import tv_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are TVTensors?\n\nTVTensors are zero-copy tensor subclasses:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensor = torch.rand(3, 256, 256)\nimage = tv_tensors.Image(tensor)\n\nassert isinstance(image, torch.Tensor)\nassert image.data_ptr() == tensor.data_ptr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under the hood, they are needed in :mod:`torchvision.transforms.v2` to correctly dispatch to the appropriate function\nfor the input data.\n\n:mod:`torchvision.tv_tensors` supports five types of TVTensors:\n\n* :class:`~torchvision.tv_tensors.Image`\n* :class:`~torchvision.tv_tensors.Video`\n* :class:`~torchvision.tv_tensors.BoundingBoxes`\n* :class:`~torchvision.tv_tensors.KeyPoints`\n* :class:`~torchvision.tv_tensors.Mask`\n\n## What can I do with a TVTensor?\n\nTVTensors look and feel just like regular tensors - they **are** tensors.\nEverything that is supported on a plain :class:`torch.Tensor` like ``.sum()`` or\nany ``torch.*`` operator will also work on TVTensors. See\n`tv_tensor_unwrapping_behaviour` for a few gotchas.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## How do I construct a TVTensor?\n\n### Using the constructor\n\nEach TVTensor class takes any tensor-like data that can be turned into a :class:`~torch.Tensor`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = tv_tensors.Image([[[[0, 1], [1, 0]]]])\nprint(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to other PyTorch creations ops, the constructor also takes the ``dtype``, ``device``, and ``requires_grad``\nparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "float_image = tv_tensors.Image([[[0, 1], [1, 0]]], dtype=torch.float32, requires_grad=True)\nprint(float_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition, :class:`~torchvision.tv_tensors.Image` and :class:`~torchvision.tv_tensors.Mask` can also take a\n:class:`PIL.Image.Image` directly:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = tv_tensors.Image(PIL.Image.open(\"../assets/astronaut.jpg\"))\nprint(image.shape, image.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some TVTensors require additional metadata to be passed in ordered to be constructed. For example,\n:class:`~torchvision.tv_tensors.BoundingBoxes` requires the coordinate format as well as the size of the\ncorresponding image (``canvas_size``) alongside the actual values. These\nmetadata are required to properly transform the bounding boxes.\nIn a similar fashion, :class:`~torchvision.tv_tensors.KeyPoints` also require the ``canvas_size`` metadata to be added.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bboxes = tv_tensors.BoundingBoxes(\n    [[17, 16, 344, 495], [0, 10, 0, 10]],\n    format=tv_tensors.BoundingBoxFormat.XYXY,\n    canvas_size=image.shape[-2:]\n)\nprint(bboxes)\n\n\nkeypoints = tv_tensors.KeyPoints(\n    [[17, 16], [344, 495], [0, 10], [0, 10]],\n    canvas_size=image.shape[-2:]\n)\nprint(keypoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using ``tv_tensors.wrap()``\n\nYou can also use the :func:`~torchvision.tv_tensors.wrap` function to wrap a tensor object\ninto a TVTensor. This is useful when you already have an object of the\ndesired type, which typically happens when writing transforms: you just want\nto wrap the output like the input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_bboxes = torch.tensor([0, 20, 30, 40])\nnew_bboxes = tv_tensors.wrap(new_bboxes, like=bboxes)\nassert isinstance(new_bboxes, tv_tensors.BoundingBoxes)\nassert new_bboxes.canvas_size == bboxes.canvas_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The metadata of ``new_bboxes`` is the same as ``bboxes``, but you could pass\nit as a parameter to override it.\n\n\n## I had a TVTensor but now I have a Tensor. Help!\n\nBy default, operations on :class:`~torchvision.tv_tensors.TVTensor` objects\nwill return a pure Tensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert isinstance(bboxes, tv_tensors.BoundingBoxes)\n\n# Shift bboxes by 3 pixels in both H and W\nnew_bboxes = bboxes + 3\n\nassert isinstance(new_bboxes, torch.Tensor)\nassert not isinstance(new_bboxes, tv_tensors.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This behavior only affects native ``torch`` operations. If you are using\n   the built-in ``torchvision`` transforms or functionals, you will always get\n   as output the same type that you passed as input (pure ``Tensor`` or\n   ``TVTensor``).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### But I want a TVTensor back!\n\nYou can re-wrap a pure tensor into a TVTensor by just calling the TVTensor\nconstructor, or by using the :func:`~torchvision.tv_tensors.wrap` function\n(see more details above in `tv_tensor_creation`):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_bboxes = bboxes + 3\nnew_bboxes = tv_tensors.wrap(new_bboxes, like=bboxes)\nassert isinstance(new_bboxes, tv_tensors.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, you can use the :func:`~torchvision.tv_tensors.set_return_type`\nas a global config setting for the whole program, or as a context manager\n(read its docs to learn more about caveats):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with tv_tensors.set_return_type(\"TVTensor\"):\n    new_bboxes = bboxes + 3\nassert isinstance(new_bboxes, tv_tensors.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why is this happening?\n\n**For performance reasons**. :class:`~torchvision.tv_tensors.TVTensor`\nclasses are Tensor subclasses, so any operation involving a\n:class:`~torchvision.tv_tensors.TVTensor` object will go through the\n[__torch_function__](https://pytorch.org/docs/stable/notes/extending.html#extending-torch)\nprotocol. This induces a small overhead, which we want to avoid when possible.\nThis doesn't matter for built-in ``torchvision`` transforms because we can\navoid the overhead there, but it could be a problem in your model's\n``forward``.\n\n**The alternative isn't much better anyway.** For every operation where\npreserving the :class:`~torchvision.tv_tensors.TVTensor` type makes\nsense, there are just as many operations where returning a pure Tensor is\npreferable: for example, is ``img.sum()`` still an :class:`~torchvision.tv_tensors.Image`?\nIf we were to preserve :class:`~torchvision.tv_tensors.TVTensor` types all\nthe way, even model's logits or the output of the loss function would end up\nbeing of type :class:`~torchvision.tv_tensors.Image`, and surely that's not\ndesirable.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This behaviour is something we're actively seeking feedback on. If you find this surprising or if you\n   have any suggestions on how to better support your use-cases, please reach out to us via this issue:\n   https://github.com/pytorch/vision/issues/7319</p></div>\n\n### Exceptions\n\nThere are a few exceptions to this \"unwrapping\" rule:\n:meth:`~torch.Tensor.clone`, :meth:`~torch.Tensor.to`,\n:meth:`torch.Tensor.detach`, and :meth:`~torch.Tensor.requires_grad_` retain\nthe TVTensor type.\n\nInplace operations on TVTensors like ``obj.add_()`` will preserve the type of\n``obj``. However, the **returned** value of inplace operations will be a pure\ntensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = tv_tensors.Image([[[0, 1], [1, 0]]])\n\nnew_image = image.add_(1).mul_(2)\n\n# image got transformed in-place and is still a TVTensor Image, but new_image\n# is a Tensor. They share the same underlying data and they're equal, just\n# different classes.\nassert isinstance(image, tv_tensors.Image)\nprint(image)\n\nassert isinstance(new_image, torch.Tensor) and not isinstance(new_image, tv_tensors.Image)\nassert (new_image == image).all()\nassert new_image.data_ptr() == image.data_ptr()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}