{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to write your own TVTensor class\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [collab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_custom_tv_tensors.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_transforms_plot_custom_tv_tensors.py>` to download the full example code.</p></div>\n\nThis guide is intended for advanced users and downstream library maintainers. We explain how to\nwrite your own TVTensor class, and how to make it compatible with the built-in\nTorchvision v2 transforms. Before continuing, make sure you have read\n`sphx_glr_auto_examples_transforms_plot_tv_tensors.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create a very simple class that just inherits from the base\n:class:`~torchvision.tv_tensors.TVTensor` class. It will be enough to cover\nwhat you need to know to implement your more elaborate uses-cases. If you need\nto create a class that carries meta-data, take a look at how the\n:class:`~torchvision.tv_tensors.BoundingBoxes` class is [implemented](https://github.com/pytorch/vision/blob/main/torchvision/tv_tensors/_bounding_box.py).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyTVTensor(tv_tensors.TVTensor):\n    pass\n\n\nmy_dp = MyTVTensor([1, 2, 3])\nmy_dp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have defined our custom TVTensor class, we want it to be\ncompatible with the built-in torchvision transforms, and the functional API.\nFor that, we need to implement a kernel which performs the core of the\ntransformation, and then \"hook\" it to the functional that we want to support\nvia :func:`~torchvision.transforms.v2.functional.register_kernel`.\n\nWe illustrate this process below: we create a kernel for the \"horizontal flip\"\noperation of our MyTVTensor class, and register it to the functional API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.v2 import functional as F\n\n\n@F.register_kernel(functional=\"hflip\", tv_tensor_cls=MyTVTensor)\ndef hflip_my_tv_tensor(my_dp, *args, **kwargs):\n    print(\"Flipping!\")\n    out = my_dp.flip(-1)\n    return tv_tensors.wrap(out, like=my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To understand why :func:`~torchvision.tv_tensors.wrap` is used, see\n`tv_tensor_unwrapping_behaviour`. Ignore the ``*args, **kwargs`` for now,\nwe will explain it below in `param_forwarding`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In our call to ``register_kernel`` above we used a string\n    ``functional=\"hflip\"`` to refer to the functional we want to hook into. We\n    could also have used the  functional *itself*, i.e.\n    ``@register_kernel(functional=F.hflip, ...)``.</p></div>\n\nNow that we have registered our kernel, we can call the functional API on a\n``MyTVTensor`` instance:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_dp = MyTVTensor(torch.rand(3, 256, 256))\n_ = F.hflip(my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can also use the\n:class:`~torchvision.transforms.v2.RandomHorizontalFlip` transform, since it relies on :func:`~torchvision.transforms.v2.functional.hflip` internally:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t = v2.RandomHorizontalFlip(p=1)\n_ = t(my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>We cannot register a kernel for a transform class, we can only register a\n    kernel for a **functional**. The reason we can't register a transform\n    class is because one transform may internally rely on more than one\n    functional, so in general we can't register a single kernel for a given\n    class.</p></div>\n\n\n## Parameter forwarding, and ensuring future compatibility of your kernels\n\nThe functional API that you're hooking into is public and therefore\n**backward** compatible: we guarantee that the parameters of these functionals\nwon't be removed or renamed without a proper deprecation cycle. However, we\ndon't guarantee **forward** compatibility, and we may add new parameters in\nthe future.\n\nImagine that in a future version, Torchvision adds a new ``inplace`` parameter\nto its :func:`~torchvision.transforms.v2.functional.hflip` functional. If you\nalready defined and registered your own kernel as\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def hflip_my_tv_tensor(my_dp):  # noqa\n    print(\"Flipping!\")\n    out = my_dp.flip(-1)\n    return tv_tensors.wrap(out, like=my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "then calling ``F.hflip(my_dp)`` will **fail**, because ``hflip`` will try to\npass the new ``inplace`` parameter to your kernel, but your kernel doesn't\naccept it.\n\nFor this reason, we recommend to always define your kernels with\n``*args, **kwargs`` in their signature, as done above. This way, your kernel\nwill be able to accept any new parameter that we may add in the future.\n(Technically, adding `**kwargs` only should be enough).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}