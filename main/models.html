


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Models and pre-trained weights &mdash; Torchvision main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom_torchvision.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="get_model" href="generated/torchvision.models.get_model.html" />
    <link rel="prev" title="wrap" href="generated/torchvision.tv_tensors.wrap.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/vision/versions.html'>main (0.17.0a0+d78b462) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Transforming and augmenting images</a></li>
<li class="toctree-l1"><a class="reference internal" href="tv_tensors.html">TVTensors</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models and pre-trained weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="ops.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="io.html">Decoding / Encoding images and videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_extraction.html">Feature extraction for model inspection</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples and training references</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Examples and tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_references.html">Training references</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/docs">PyTorch</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Models and pre-trained weights</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/models.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="models-and-pre-trained-weights">
<span id="models"></span><h1>Models and pre-trained weights<a class="headerlink" href="#models-and-pre-trained-weights" title="Permalink to this heading">¶</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> subpackage contains definitions of models for addressing
different tasks, including: image classification, pixelwise semantic
segmentation, object detection, instance segmentation, person
keypoint detection, video classification, and optical flow.</p>
<section id="general-information-on-pre-trained-weights">
<h2>General information on pre-trained weights<a class="headerlink" href="#general-information-on-pre-trained-weights" title="Permalink to this heading">¶</a></h2>
<p>TorchVision offers pre-trained weights for every provided architecture, using
the PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/hub.html#module-torch.hub" title="(in PyTorch v2.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.hub</span></code></a>. Instancing a pre-trained model will download its
weights to a cache directory. This directory can be set using the <cite>TORCH_HOME</cite>
environment variable. See <a class="reference external" href="https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url" title="(in PyTorch v2.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hub.load_state_dict_from_url()</span></code></a> for details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The pre-trained models provided in this library may have their own licenses or
terms and conditions derived from the dataset used for training. It is your
responsibility to determine whether you have permission to use the models for
your use case.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Backward compatibility is guaranteed for loading a serialized
<code class="docutils literal notranslate"><span class="pre">state_dict</span></code> to the model created using old PyTorch version.
On the contrary, loading entire saved models or serialized
<code class="docutils literal notranslate"><span class="pre">ScriptModules</span></code> (serialized using older versions of PyTorch)
may not preserve the historic behaviour. Refer to the following
<a class="reference external" href="https://pytorch.org/docs/stable/notes/serialization.html#id6">documentation</a></p>
</div>
<section id="initializing-pre-trained-models">
<h3>Initializing pre-trained models<a class="headerlink" href="#initializing-pre-trained-models" title="Permalink to this heading">¶</a></h3>
<p>As of v0.13, TorchVision offers a new <a class="reference external" href="https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/">Multi-weight support API</a>
for loading different weights to the existing model builder methods:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="c1"># Old weights with accuracy 76.130%</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>

<span class="c1"># New weights with accuracy 80.858%</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V2</span><span class="p">)</span>

<span class="c1"># Best available weights (currently alias for IMAGENET1K_V2)</span>
<span class="c1"># Note that these weights may change across versions</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">)</span>

<span class="c1"># Strings are also supported</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V2&quot;</span><span class="p">)</span>

<span class="c1"># No weights - random initialization</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Migrating to the new API is very straightforward. The following method calls between the 2 APIs are all equivalent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="c1"># Using pretrained weights:</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V1&quot;</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># deprecated</span>
<span class="n">resnet50</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># deprecated</span>

<span class="c1"># Using no weights:</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">()</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># deprecated</span>
<span class="n">resnet50</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># deprecated</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">pretrained</span></code> parameter is now deprecated, using it will emit warnings and will be removed on v0.15.</p>
</section>
<section id="using-the-pre-trained-models">
<h3>Using the pre-trained models<a class="headerlink" href="#using-the-pre-trained-models" title="Permalink to this heading">¶</a></h3>
<p>Before using the pre-trained models, one must preprocess the image
(resize with right resolution/interpolation, apply inference transforms,
rescale the values etc). There is no standard way to do this as it depends on
how a given model was trained. It can vary across model families, variants or
even weight versions. Using the correct preprocessing method is critical and
failing to do so may lead to decreased accuracy or incorrect outputs.</p>
<p>All the necessary information for the inference transforms of each pre-trained
model is provided on its weights documentation. To simplify inference, TorchVision
bundles the necessary preprocessing transforms into each model weight. These are
accessible via the <code class="docutils literal notranslate"><span class="pre">weight.transforms</span></code> attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize the Weight Transforms</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Apply it to the input image</span>
<span class="n">img_transformed</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p>Some models use modules which have different training and evaluation
behavior, such as batch normalization. To switch between these modes, use
<code class="docutils literal notranslate"><span class="pre">model.train()</span></code> or <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> as appropriate. See
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train" title="(in PyTorch v2.0)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train()</span></code></a> or <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval" title="(in PyTorch v2.0)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">eval()</span></code></a> for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize model</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>

<span class="c1"># Set model to eval mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="listing-and-retrieving-available-models">
<h3>Listing and retrieving available models<a class="headerlink" href="#listing-and-retrieving-available-models" title="Permalink to this heading">¶</a></h3>
<p>As of v0.14, TorchVision offers a new mechanism which allows listing and
retrieving models and weights by their names. Here are a few examples on how to
use them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># List available models</span>
<span class="n">all_models</span> <span class="o">=</span> <span class="n">list_models</span><span class="p">()</span>
<span class="n">classification_models</span> <span class="o">=</span> <span class="n">list_models</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>

<span class="c1"># Initialize models</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="s2">&quot;mobilenet_v3_large&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">m2</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="s2">&quot;quantized_mobilenet_v3_large&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;DEFAULT&quot;</span><span class="p">)</span>

<span class="c1"># Fetch weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">get_weight</span><span class="p">(</span><span class="s2">&quot;MobileNet_V3_Large_QuantizedWeights.DEFAULT&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">weights</span> <span class="o">==</span> <span class="n">MobileNet_V3_Large_QuantizedWeights</span><span class="o">.</span><span class="n">DEFAULT</span>

<span class="n">weights_enum</span> <span class="o">=</span> <span class="n">get_model_weights</span><span class="p">(</span><span class="s2">&quot;quantized_mobilenet_v3_large&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">weights_enum</span> <span class="o">==</span> <span class="n">MobileNet_V3_Large_QuantizedWeights</span>

<span class="n">weights_enum2</span> <span class="o">=</span> <span class="n">get_model_weights</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">mobilenet_v3_large</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">weights_enum</span> <span class="o">==</span> <span class="n">weights_enum2</span>
</pre></div>
</div>
<p>Here are the available public functions to retrieve models and their corresponding weights:</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.get_model.html#torchvision.models.get_model" title="torchvision.models.get_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model</span></code></a>(name, **config)</p></td>
<td><p>Gets the model name and configuration and returns an instantiated model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.get_model_weights.html#torchvision.models.get_model_weights" title="torchvision.models.get_model_weights"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model_weights</span></code></a>(name)</p></td>
<td><p>Returns the weights enum class associated to the given model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchvision.models.get_weight.html#torchvision.models.get_weight" title="torchvision.models.get_weight"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_weight</span></code></a>(name)</p></td>
<td><p>Gets the weights enum value by its full name.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchvision.models.list_models.html#torchvision.models.list_models" title="torchvision.models.list_models"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list_models</span></code></a>([module, include, exclude])</p></td>
<td><p>Returns a list with the names of registered models.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="using-models-from-hub">
<h3>Using models from Hub<a class="headerlink" href="#using-models-from-hub" title="Permalink to this heading">¶</a></h3>
<p>Most pre-trained models can be accessed directly via PyTorch Hub without having TorchVision installed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Option 1: passing weights param as string</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V2&quot;</span><span class="p">)</span>

<span class="c1"># Option 2: passing weights param as enum</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;get_weight&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;ResNet50_Weights.IMAGENET1K_V2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also retrieve all the available weights of a specific model via PyTorch Hub by doing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">weight_enum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;get_model_weights&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;resnet50&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">weight</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weight_enum</span><span class="p">])</span>
</pre></div>
</div>
<p>The only exception to the above are the detection models included on
<code class="xref py py-mod docutils literal notranslate"><span class="pre">torchvision.models.detection</span></code>. These models require TorchVision
to be installed because they depend on custom C++ operators.</p>
</section>
</section>
<section id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this heading">¶</a></h2>
<p>The following classification models are available, with or without pre-trained
weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/alexnet.html">AlexNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/convnext.html">ConvNeXt</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/densenet.html">DenseNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/efficientnet.html">EfficientNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/efficientnetv2.html">EfficientNetV2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/googlenet.html">GoogLeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/inception.html">Inception V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/maxvit.html">MaxVit</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mnasnet.html">MNASNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv2.html">MobileNet V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv3.html">MobileNet V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/regnet.html">RegNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnet.html">ResNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnext.html">ResNeXt</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/shufflenetv2.html">ShuffleNet V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/squeezenet.html">SqueezeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/swin_transformer.html">SwinTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/vgg.html">VGG</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/vision_transformer.html">VisionTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/wide_resnet.html">Wide ResNet</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained image classification models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;test/assets/encode_jpeg/grace_hopper_517x606.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">class_id</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">class_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">class_id</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">score</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.</p>
<section id="table-of-all-available-classification-weights">
<h3>Table of all available classification weights<a class="headerlink" href="#table-of-all-available-classification-weights" title="Permalink to this heading">¶</a></h3>
<p>Accuracies are reported on ImageNet-1K using single crops:</p>
<table class="table-weights docutils align-default">
<colgroup>
<col style="width: 57%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Weight</strong></p></th>
<th class="head"><p><strong>Acc&#64;1</strong></p></th>
<th class="head"><p><strong>Acc&#64;5</strong></p></th>
<th class="head"><p><strong>Params</strong></p></th>
<th class="head"><p><strong>GFLOPS</strong></p></th>
<th class="head"><p><strong>Recipe</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.alexnet.html#torchvision.models.AlexNet_Weights" title="torchvision.models.AlexNet_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlexNet_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>56.522</p></td>
<td><p>79.066</p></td>
<td><p>61.1M</p></td>
<td><p>0.71</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.convnext_base.html#torchvision.models.ConvNeXt_Base_Weights" title="torchvision.models.ConvNeXt_Base_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvNeXt_Base_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>84.062</p></td>
<td><p>96.87</p></td>
<td><p>88.6M</p></td>
<td><p>15.36</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#convnext">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.convnext_large.html#torchvision.models.ConvNeXt_Large_Weights" title="torchvision.models.ConvNeXt_Large_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvNeXt_Large_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>84.414</p></td>
<td><p>96.976</p></td>
<td><p>197.8M</p></td>
<td><p>34.36</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#convnext">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.convnext_small.html#torchvision.models.ConvNeXt_Small_Weights" title="torchvision.models.ConvNeXt_Small_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvNeXt_Small_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.616</p></td>
<td><p>96.65</p></td>
<td><p>50.2M</p></td>
<td><p>8.68</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#convnext">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.convnext_tiny.html#torchvision.models.ConvNeXt_Tiny_Weights" title="torchvision.models.ConvNeXt_Tiny_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvNeXt_Tiny_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>82.52</p></td>
<td><p>96.146</p></td>
<td><p>28.6M</p></td>
<td><p>4.46</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#convnext">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.densenet121.html#torchvision.models.DenseNet121_Weights" title="torchvision.models.DenseNet121_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseNet121_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>74.434</p></td>
<td><p>91.972</p></td>
<td><p>8.0M</p></td>
<td><p>2.83</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/116">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.densenet161.html#torchvision.models.DenseNet161_Weights" title="torchvision.models.DenseNet161_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseNet161_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>77.138</p></td>
<td><p>93.56</p></td>
<td><p>28.7M</p></td>
<td><p>7.73</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/116">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.densenet169.html#torchvision.models.DenseNet169_Weights" title="torchvision.models.DenseNet169_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseNet169_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>75.6</p></td>
<td><p>92.806</p></td>
<td><p>14.1M</p></td>
<td><p>3.36</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/116">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.densenet201.html#torchvision.models.DenseNet201_Weights" title="torchvision.models.DenseNet201_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">DenseNet201_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>76.896</p></td>
<td><p>93.37</p></td>
<td><p>20.0M</p></td>
<td><p>4.29</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/116">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights" title="torchvision.models.EfficientNet_B0_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B0_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>77.692</p></td>
<td><p>93.532</p></td>
<td><p>5.3M</p></td>
<td><p>0.39</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights" title="torchvision.models.EfficientNet_B1_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B1_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>78.642</p></td>
<td><p>94.186</p></td>
<td><p>7.8M</p></td>
<td><p>0.69</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b1.html#torchvision.models.EfficientNet_B1_Weights" title="torchvision.models.EfficientNet_B1_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B1_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>79.838</p></td>
<td><p>94.934</p></td>
<td><p>7.8M</p></td>
<td><p>0.69</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-lr-wd-crop-tuning">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights" title="torchvision.models.EfficientNet_B2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B2_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>80.608</p></td>
<td><p>95.31</p></td>
<td><p>9.1M</p></td>
<td><p>1.09</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b3.html#torchvision.models.EfficientNet_B3_Weights" title="torchvision.models.EfficientNet_B3_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B3_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>82.008</p></td>
<td><p>96.054</p></td>
<td><p>12.2M</p></td>
<td><p>1.83</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b4.html#torchvision.models.EfficientNet_B4_Weights" title="torchvision.models.EfficientNet_B4_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B4_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.384</p></td>
<td><p>96.594</p></td>
<td><p>19.3M</p></td>
<td><p>4.39</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b5.html#torchvision.models.EfficientNet_B5_Weights" title="torchvision.models.EfficientNet_B5_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B5_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.444</p></td>
<td><p>96.628</p></td>
<td><p>30.4M</p></td>
<td><p>10.27</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b6.html#torchvision.models.EfficientNet_B6_Weights" title="torchvision.models.EfficientNet_B6_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B6_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>84.008</p></td>
<td><p>96.916</p></td>
<td><p>43.0M</p></td>
<td><p>19.07</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_b7.html#torchvision.models.EfficientNet_B7_Weights" title="torchvision.models.EfficientNet_B7_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_B7_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>84.122</p></td>
<td><p>96.908</p></td>
<td><p>66.3M</p></td>
<td><p>37.75</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v1">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_v2_l.html#torchvision.models.EfficientNet_V2_L_Weights" title="torchvision.models.EfficientNet_V2_L_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_V2_L_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>85.808</p></td>
<td><p>97.788</p></td>
<td><p>118.5M</p></td>
<td><p>56.08</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_v2_m.html#torchvision.models.EfficientNet_V2_M_Weights" title="torchvision.models.EfficientNet_V2_M_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_V2_M_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>85.112</p></td>
<td><p>97.156</p></td>
<td><p>54.1M</p></td>
<td><p>24.58</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.EfficientNet_V2_S_Weights" title="torchvision.models.EfficientNet_V2_S_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">EfficientNet_V2_S_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>84.228</p></td>
<td><p>96.878</p></td>
<td><p>21.5M</p></td>
<td><p>8.37</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#efficientnet-v2">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.googlenet.html#torchvision.models.GoogLeNet_Weights" title="torchvision.models.GoogLeNet_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">GoogLeNet_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>69.778</p></td>
<td><p>89.53</p></td>
<td><p>6.6M</p></td>
<td><p>1.5</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#googlenet">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.inception_v3.html#torchvision.models.Inception_V3_Weights" title="torchvision.models.Inception_V3_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Inception_V3_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>77.294</p></td>
<td><p>93.45</p></td>
<td><p>27.2M</p></td>
<td><p>5.71</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#inception-v3">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.mnasnet0_5.html#torchvision.models.MNASNet0_5_Weights" title="torchvision.models.MNASNet0_5_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MNASNet0_5_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>67.734</p></td>
<td><p>87.49</p></td>
<td><p>2.2M</p></td>
<td><p>0.1</p></td>
<td><p><a class="reference external" href="https://github.com/1e100/mnasnet_trainer">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.mnasnet0_75.html#torchvision.models.MNASNet0_75_Weights" title="torchvision.models.MNASNet0_75_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MNASNet0_75_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>71.18</p></td>
<td><p>90.496</p></td>
<td><p>3.2M</p></td>
<td><p>0.21</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/6019">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.mnasnet1_0.html#torchvision.models.MNASNet1_0_Weights" title="torchvision.models.MNASNet1_0_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MNASNet1_0_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>73.456</p></td>
<td><p>91.51</p></td>
<td><p>4.4M</p></td>
<td><p>0.31</p></td>
<td><p><a class="reference external" href="https://github.com/1e100/mnasnet_trainer">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.mnasnet1_3.html#torchvision.models.MNASNet1_3_Weights" title="torchvision.models.MNASNet1_3_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MNASNet1_3_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>76.506</p></td>
<td><p>93.522</p></td>
<td><p>6.3M</p></td>
<td><p>0.53</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/6019">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.maxvit_t.html#torchvision.models.MaxVit_T_Weights" title="torchvision.models.MaxVit_T_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxVit_T_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.7</p></td>
<td><p>96.722</p></td>
<td><p>30.9M</p></td>
<td><p>5.56</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#maxvit">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights" title="torchvision.models.MobileNet_V2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileNet_V2_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>71.878</p></td>
<td><p>90.286</p></td>
<td><p>3.5M</p></td>
<td><p>0.3</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights" title="torchvision.models.MobileNet_V2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileNet_V2_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>72.154</p></td>
<td><p>90.822</p></td>
<td><p>3.5M</p></td>
<td><p>0.3</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights" title="torchvision.models.MobileNet_V3_Large_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileNet_V3_Large_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>74.042</p></td>
<td><p>91.34</p></td>
<td><p>5.5M</p></td>
<td><p>0.22</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.MobileNet_V3_Large_Weights" title="torchvision.models.MobileNet_V3_Large_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileNet_V3_Large_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>75.274</p></td>
<td><p>92.566</p></td>
<td><p>5.5M</p></td>
<td><p>0.22</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.MobileNet_V3_Small_Weights" title="torchvision.models.MobileNet_V3_Small_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileNet_V3_Small_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>67.668</p></td>
<td><p>87.402</p></td>
<td><p>2.5M</p></td>
<td><p>0.06</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv3-large--small">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights" title="torchvision.models.RegNet_X_16GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_16GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>80.058</p></td>
<td><p>94.944</p></td>
<td><p>54.3M</p></td>
<td><p>15.94</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#medium-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_16gf.html#torchvision.models.RegNet_X_16GF_Weights" title="torchvision.models.RegNet_X_16GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_16GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>82.716</p></td>
<td><p>96.196</p></td>
<td><p>54.3M</p></td>
<td><p>15.94</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights" title="torchvision.models.RegNet_X_1_6GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_1_6GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>77.04</p></td>
<td><p>93.44</p></td>
<td><p>9.2M</p></td>
<td><p>1.6</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#small-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_1_6gf.html#torchvision.models.RegNet_X_1_6GF_Weights" title="torchvision.models.RegNet_X_1_6GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_1_6GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>79.668</p></td>
<td><p>94.922</p></td>
<td><p>9.2M</p></td>
<td><p>1.6</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights" title="torchvision.models.RegNet_X_32GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_32GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>80.622</p></td>
<td><p>95.248</p></td>
<td><p>107.8M</p></td>
<td><p>31.74</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#large-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_32gf.html#torchvision.models.RegNet_X_32GF_Weights" title="torchvision.models.RegNet_X_32GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_32GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>83.014</p></td>
<td><p>96.288</p></td>
<td><p>107.8M</p></td>
<td><p>31.74</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights" title="torchvision.models.RegNet_X_3_2GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_3_2GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>78.364</p></td>
<td><p>93.992</p></td>
<td><p>15.3M</p></td>
<td><p>3.18</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#medium-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_3_2gf.html#torchvision.models.RegNet_X_3_2GF_Weights" title="torchvision.models.RegNet_X_3_2GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_3_2GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>81.196</p></td>
<td><p>95.43</p></td>
<td><p>15.3M</p></td>
<td><p>3.18</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights" title="torchvision.models.RegNet_X_400MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_400MF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>72.834</p></td>
<td><p>90.95</p></td>
<td><p>5.5M</p></td>
<td><p>0.41</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#small-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_400mf.html#torchvision.models.RegNet_X_400MF_Weights" title="torchvision.models.RegNet_X_400MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_400MF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>74.864</p></td>
<td><p>92.322</p></td>
<td><p>5.5M</p></td>
<td><p>0.41</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights" title="torchvision.models.RegNet_X_800MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_800MF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>75.212</p></td>
<td><p>92.348</p></td>
<td><p>7.3M</p></td>
<td><p>0.8</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#small-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_800mf.html#torchvision.models.RegNet_X_800MF_Weights" title="torchvision.models.RegNet_X_800MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_800MF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>77.522</p></td>
<td><p>93.826</p></td>
<td><p>7.3M</p></td>
<td><p>0.8</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights" title="torchvision.models.RegNet_X_8GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_8GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>79.344</p></td>
<td><p>94.686</p></td>
<td><p>39.6M</p></td>
<td><p>8</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#medium-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_x_8gf.html#torchvision.models.RegNet_X_8GF_Weights" title="torchvision.models.RegNet_X_8GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_X_8GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>81.682</p></td>
<td><p>95.678</p></td>
<td><p>39.6M</p></td>
<td><p>8</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights" title="torchvision.models.RegNet_Y_128GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>
<td><p>88.228</p></td>
<td><p>98.682</p></td>
<td><p>644.8M</p></td>
<td><p>374.57</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/SWAG">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights" title="torchvision.models.RegNet_Y_128GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>
<td><p>86.068</p></td>
<td><p>97.844</p></td>
<td><p>644.8M</p></td>
<td><p>127.52</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5793">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights" title="torchvision.models.RegNet_Y_16GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_16GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>80.424</p></td>
<td><p>95.24</p></td>
<td><p>83.6M</p></td>
<td><p>15.91</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#large-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights" title="torchvision.models.RegNet_Y_16GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_16GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>82.886</p></td>
<td><p>96.328</p></td>
<td><p>83.6M</p></td>
<td><p>15.91</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights" title="torchvision.models.RegNet_Y_16GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>
<td><p>86.012</p></td>
<td><p>98.054</p></td>
<td><p>83.6M</p></td>
<td><p>46.73</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/SWAG">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_16gf.html#torchvision.models.RegNet_Y_16GF_Weights" title="torchvision.models.RegNet_Y_16GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>
<td><p>83.976</p></td>
<td><p>97.244</p></td>
<td><p>83.6M</p></td>
<td><p>15.91</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5793">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights" title="torchvision.models.RegNet_Y_1_6GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_1_6GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>77.95</p></td>
<td><p>93.966</p></td>
<td><p>11.2M</p></td>
<td><p>1.61</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#small-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_1_6gf.html#torchvision.models.RegNet_Y_1_6GF_Weights" title="torchvision.models.RegNet_Y_1_6GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_1_6GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>80.876</p></td>
<td><p>95.444</p></td>
<td><p>11.2M</p></td>
<td><p>1.61</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights" title="torchvision.models.RegNet_Y_32GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_32GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>80.878</p></td>
<td><p>95.34</p></td>
<td><p>145.0M</p></td>
<td><p>32.28</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#large-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights" title="torchvision.models.RegNet_Y_32GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_32GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>83.368</p></td>
<td><p>96.498</p></td>
<td><p>145.0M</p></td>
<td><p>32.28</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights" title="torchvision.models.RegNet_Y_32GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>
<td><p>86.838</p></td>
<td><p>98.362</p></td>
<td><p>145.0M</p></td>
<td><p>94.83</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/SWAG">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_32gf.html#torchvision.models.RegNet_Y_32GF_Weights" title="torchvision.models.RegNet_Y_32GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>
<td><p>84.622</p></td>
<td><p>97.48</p></td>
<td><p>145.0M</p></td>
<td><p>32.28</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5793">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights" title="torchvision.models.RegNet_Y_3_2GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_3_2GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>78.948</p></td>
<td><p>94.576</p></td>
<td><p>19.4M</p></td>
<td><p>3.18</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#medium-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_3_2gf.html#torchvision.models.RegNet_Y_3_2GF_Weights" title="torchvision.models.RegNet_Y_3_2GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_3_2GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>81.982</p></td>
<td><p>95.972</p></td>
<td><p>19.4M</p></td>
<td><p>3.18</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights" title="torchvision.models.RegNet_Y_400MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_400MF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>74.046</p></td>
<td><p>91.716</p></td>
<td><p>4.3M</p></td>
<td><p>0.4</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#small-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_400mf.html#torchvision.models.RegNet_Y_400MF_Weights" title="torchvision.models.RegNet_Y_400MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_400MF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>75.804</p></td>
<td><p>92.742</p></td>
<td><p>4.3M</p></td>
<td><p>0.4</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights" title="torchvision.models.RegNet_Y_800MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_800MF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>76.42</p></td>
<td><p>93.136</p></td>
<td><p>6.4M</p></td>
<td><p>0.83</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#small-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_800mf.html#torchvision.models.RegNet_Y_800MF_Weights" title="torchvision.models.RegNet_Y_800MF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_800MF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>78.828</p></td>
<td><p>94.502</p></td>
<td><p>6.4M</p></td>
<td><p>0.83</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights" title="torchvision.models.RegNet_Y_8GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_8GF_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>80.032</p></td>
<td><p>95.048</p></td>
<td><p>39.4M</p></td>
<td><p>8.47</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#medium-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.regnet_y_8gf.html#torchvision.models.RegNet_Y_8GF_Weights" title="torchvision.models.RegNet_Y_8GF_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegNet_Y_8GF_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>82.828</p></td>
<td><p>96.33</p></td>
<td><p>39.4M</p></td>
<td><p>8.47</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights" title="torchvision.models.ResNeXt101_32X8D_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt101_32X8D_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>79.312</p></td>
<td><p>94.526</p></td>
<td><p>88.8M</p></td>
<td><p>16.41</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#resnext">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnext101_32x8d.html#torchvision.models.ResNeXt101_32X8D_Weights" title="torchvision.models.ResNeXt101_32X8D_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt101_32X8D_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>82.834</p></td>
<td><p>96.228</p></td>
<td><p>88.8M</p></td>
<td><p>16.41</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnext101_64x4d.html#torchvision.models.ResNeXt101_64X4D_Weights" title="torchvision.models.ResNeXt101_64X4D_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt101_64X4D_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.246</p></td>
<td><p>96.454</p></td>
<td><p>83.5M</p></td>
<td><p>15.46</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5935">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights" title="torchvision.models.ResNeXt50_32X4D_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt50_32X4D_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>77.618</p></td>
<td><p>93.698</p></td>
<td><p>25.0M</p></td>
<td><p>4.23</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#resnext">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.ResNeXt50_32X4D_Weights" title="torchvision.models.ResNeXt50_32X4D_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt50_32X4D_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>81.198</p></td>
<td><p>95.34</p></td>
<td><p>25.0M</p></td>
<td><p>4.23</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights" title="torchvision.models.ResNet101_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet101_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>77.374</p></td>
<td><p>93.546</p></td>
<td><p>44.5M</p></td>
<td><p>7.8</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#resnet">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights" title="torchvision.models.ResNet101_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet101_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>81.886</p></td>
<td><p>95.78</p></td>
<td><p>44.5M</p></td>
<td><p>7.8</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights" title="torchvision.models.ResNet152_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet152_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>78.312</p></td>
<td><p>94.046</p></td>
<td><p>60.2M</p></td>
<td><p>11.51</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#resnet">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights" title="torchvision.models.ResNet152_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet152_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>82.284</p></td>
<td><p>96.002</p></td>
<td><p>60.2M</p></td>
<td><p>11.51</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights" title="torchvision.models.ResNet18_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet18_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>69.758</p></td>
<td><p>89.078</p></td>
<td><p>11.7M</p></td>
<td><p>1.81</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#resnet">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet34.html#torchvision.models.ResNet34_Weights" title="torchvision.models.ResNet34_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet34_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>73.314</p></td>
<td><p>91.42</p></td>
<td><p>21.8M</p></td>
<td><p>3.66</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#resnet">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights" title="torchvision.models.ResNet50_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet50_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>76.13</p></td>
<td><p>92.862</p></td>
<td><p>25.6M</p></td>
<td><p>4.09</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#resnet">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights" title="torchvision.models.ResNet50_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet50_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>80.858</p></td>
<td><p>95.434</p></td>
<td><p>25.6M</p></td>
<td><p>4.09</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.shufflenet_v2_x0_5.html#torchvision.models.ShuffleNet_V2_X0_5_Weights" title="torchvision.models.ShuffleNet_V2_X0_5_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>60.552</p></td>
<td><p>81.746</p></td>
<td><p>1.4M</p></td>
<td><p>0.04</p></td>
<td><p><a class="reference external" href="https://github.com/ericsun99/Shufflenet-v2-Pytorch">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.shufflenet_v2_x1_0.html#torchvision.models.ShuffleNet_V2_X1_0_Weights" title="torchvision.models.ShuffleNet_V2_X1_0_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>69.362</p></td>
<td><p>88.316</p></td>
<td><p>2.3M</p></td>
<td><p>0.14</p></td>
<td><p><a class="reference external" href="https://github.com/ericsun99/Shufflenet-v2-Pytorch">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.shufflenet_v2_x1_5.html#torchvision.models.ShuffleNet_V2_X1_5_Weights" title="torchvision.models.ShuffleNet_V2_X1_5_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X1_5_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>72.996</p></td>
<td><p>91.086</p></td>
<td><p>3.5M</p></td>
<td><p>0.3</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5906">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.shufflenet_v2_x2_0.html#torchvision.models.ShuffleNet_V2_X2_0_Weights" title="torchvision.models.ShuffleNet_V2_X2_0_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>76.23</p></td>
<td><p>93.006</p></td>
<td><p>7.4M</p></td>
<td><p>0.58</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5906">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.squeezenet1_0.html#torchvision.models.SqueezeNet1_0_Weights" title="torchvision.models.SqueezeNet1_0_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeNet1_0_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>58.092</p></td>
<td><p>80.42</p></td>
<td><p>1.2M</p></td>
<td><p>0.82</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/49#issuecomment-277560717">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.squeezenet1_1.html#torchvision.models.SqueezeNet1_1_Weights" title="torchvision.models.SqueezeNet1_1_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">SqueezeNet1_1_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>58.178</p></td>
<td><p>80.624</p></td>
<td><p>1.2M</p></td>
<td><p>0.35</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/49#issuecomment-277560717">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.swin_b.html#torchvision.models.Swin_B_Weights" title="torchvision.models.Swin_B_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin_B_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.582</p></td>
<td><p>96.64</p></td>
<td><p>87.8M</p></td>
<td><p>15.43</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#swintransformer">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.swin_s.html#torchvision.models.Swin_S_Weights" title="torchvision.models.Swin_S_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin_S_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.196</p></td>
<td><p>96.36</p></td>
<td><p>49.6M</p></td>
<td><p>8.74</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#swintransformer">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.swin_t.html#torchvision.models.Swin_T_Weights" title="torchvision.models.Swin_T_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin_T_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>81.474</p></td>
<td><p>95.776</p></td>
<td><p>28.3M</p></td>
<td><p>4.49</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#swintransformer">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.swin_v2_b.html#torchvision.models.Swin_V2_B_Weights" title="torchvision.models.Swin_V2_B_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin_V2_B_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>84.112</p></td>
<td><p>96.864</p></td>
<td><p>87.9M</p></td>
<td><p>20.32</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.swin_v2_s.html#torchvision.models.Swin_V2_S_Weights" title="torchvision.models.Swin_V2_S_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin_V2_S_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>83.712</p></td>
<td><p>96.816</p></td>
<td><p>49.7M</p></td>
<td><p>11.55</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.swin_v2_t.html#torchvision.models.Swin_V2_T_Weights" title="torchvision.models.Swin_V2_T_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin_V2_T_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>82.072</p></td>
<td><p>96.132</p></td>
<td><p>28.4M</p></td>
<td><p>5.94</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#swintransformer-v2">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg11_bn.html#torchvision.models.VGG11_BN_Weights" title="torchvision.models.VGG11_BN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG11_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>70.37</p></td>
<td><p>89.81</p></td>
<td><p>132.9M</p></td>
<td><p>7.61</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg11.html#torchvision.models.VGG11_Weights" title="torchvision.models.VGG11_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG11_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>69.02</p></td>
<td><p>88.628</p></td>
<td><p>132.9M</p></td>
<td><p>7.61</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg13_bn.html#torchvision.models.VGG13_BN_Weights" title="torchvision.models.VGG13_BN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG13_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>71.586</p></td>
<td><p>90.374</p></td>
<td><p>133.1M</p></td>
<td><p>11.31</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg13.html#torchvision.models.VGG13_Weights" title="torchvision.models.VGG13_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG13_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>69.928</p></td>
<td><p>89.246</p></td>
<td><p>133.0M</p></td>
<td><p>11.31</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg16_bn.html#torchvision.models.VGG16_BN_Weights" title="torchvision.models.VGG16_BN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG16_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>73.36</p></td>
<td><p>91.516</p></td>
<td><p>138.4M</p></td>
<td><p>15.47</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights" title="torchvision.models.VGG16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG16_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>71.592</p></td>
<td><p>90.382</p></td>
<td><p>138.4M</p></td>
<td><p>15.47</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights" title="torchvision.models.VGG16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG16_Weights.IMAGENET1K_FEATURES</span></code></a></p></td>
<td><p>nan</p></td>
<td><p>nan</p></td>
<td><p>138.4M</p></td>
<td><p>15.47</p></td>
<td><p><a class="reference external" href="https://github.com/amdegroot/ssd.pytorch#training-ssd">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg19_bn.html#torchvision.models.VGG19_BN_Weights" title="torchvision.models.VGG19_BN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG19_BN_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>74.218</p></td>
<td><p>91.842</p></td>
<td><p>143.7M</p></td>
<td><p>19.63</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vgg19.html#torchvision.models.VGG19_Weights" title="torchvision.models.VGG19_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">VGG19_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>72.376</p></td>
<td><p>90.876</p></td>
<td><p>143.7M</p></td>
<td><p>19.63</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#alexnet-and-vgg">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights" title="torchvision.models.ViT_B_16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_B_16_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>81.072</p></td>
<td><p>95.318</p></td>
<td><p>86.6M</p></td>
<td><p>17.56</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#vit_b_16">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights" title="torchvision.models.ViT_B_16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>
<td><p>85.304</p></td>
<td><p>97.65</p></td>
<td><p>86.9M</p></td>
<td><p>55.48</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/SWAG">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights" title="torchvision.models.ViT_B_16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>
<td><p>81.886</p></td>
<td><p>96.18</p></td>
<td><p>86.6M</p></td>
<td><p>17.56</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5793">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_b_32.html#torchvision.models.ViT_B_32_Weights" title="torchvision.models.ViT_B_32_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_B_32_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>75.912</p></td>
<td><p>92.466</p></td>
<td><p>88.2M</p></td>
<td><p>4.41</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#vit_b_32">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights" title="torchvision.models.ViT_H_14_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>
<td><p>88.552</p></td>
<td><p>98.694</p></td>
<td><p>633.5M</p></td>
<td><p>1016.72</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/SWAG">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_h_14.html#torchvision.models.ViT_H_14_Weights" title="torchvision.models.ViT_H_14_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>
<td><p>85.708</p></td>
<td><p>97.73</p></td>
<td><p>632.0M</p></td>
<td><p>167.29</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5793">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights" title="torchvision.models.ViT_L_16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_L_16_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>79.662</p></td>
<td><p>94.638</p></td>
<td><p>304.3M</p></td>
<td><p>61.55</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#vit_l_16">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights" title="torchvision.models.ViT_L_16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1</span></code></a></p></td>
<td><p>88.064</p></td>
<td><p>98.512</p></td>
<td><p>305.2M</p></td>
<td><p>361.99</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/SWAG">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_l_16.html#torchvision.models.ViT_L_16_Weights" title="torchvision.models.ViT_L_16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1</span></code></a></p></td>
<td><p>85.146</p></td>
<td><p>97.422</p></td>
<td><p>304.3M</p></td>
<td><p>61.55</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5793">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.vit_l_32.html#torchvision.models.ViT_L_32_Weights" title="torchvision.models.ViT_L_32_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ViT_L_32_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>76.972</p></td>
<td><p>93.07</p></td>
<td><p>306.5M</p></td>
<td><p>15.38</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#vit_l_32">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights" title="torchvision.models.Wide_ResNet101_2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Wide_ResNet101_2_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>78.848</p></td>
<td><p>94.284</p></td>
<td><p>126.9M</p></td>
<td><p>22.75</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/912#issue-445437439">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.wide_resnet101_2.html#torchvision.models.Wide_ResNet101_2_Weights" title="torchvision.models.Wide_ResNet101_2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Wide_ResNet101_2_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>82.51</p></td>
<td><p>96.02</p></td>
<td><p>126.9M</p></td>
<td><p>22.75</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights" title="torchvision.models.Wide_ResNet50_2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Wide_ResNet50_2_Weights.IMAGENET1K_V1</span></code></a></p></td>
<td><p>78.468</p></td>
<td><p>94.086</p></td>
<td><p>68.9M</p></td>
<td><p>11.4</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/912#issue-445437439">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.wide_resnet50_2.html#torchvision.models.Wide_ResNet50_2_Weights" title="torchvision.models.Wide_ResNet50_2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Wide_ResNet50_2_Weights.IMAGENET1K_V2</span></code></a></p></td>
<td><p>81.602</p></td>
<td><p>95.758</p></td>
<td><p>68.9M</p></td>
<td><p>11.4</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/3995#new-recipe-with-fixres">link</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="quantized-models">
<h3>Quantized models<a class="headerlink" href="#quantized-models" title="Permalink to this heading">¶</a></h3>
<p>The following architectures provide support for INT8 quantized models, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/googlenet_quant.html">Quantized GoogLeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/inception_quant.html">Quantized InceptionV3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv2_quant.html">Quantized MobileNet V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv3_quant.html">Quantized MobileNet V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnet_quant.html">Quantized ResNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnext_quant.html">Quantized ResNeXt</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/shufflenetv2_quant.html">Quantized ShuffleNet V2</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained quantized image classification models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models.quantization</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_QuantizedWeights</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;test/assets/encode_jpeg/grace_hopper_517x606.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_QuantizedWeights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">class_id</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">class_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">class_id</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">score</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.</p>
<section id="table-of-all-available-quantized-classification-weights">
<h4>Table of all available quantized classification weights<a class="headerlink" href="#table-of-all-available-quantized-classification-weights" title="Permalink to this heading">¶</a></h4>
<p>Accuracies are reported on ImageNet-1K using single crops:</p>
<table class="table-weights docutils align-default">
<colgroup>
<col style="width: 57%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Weight</strong></p></th>
<th class="head"><p><strong>Acc&#64;1</strong></p></th>
<th class="head"><p><strong>Acc&#64;5</strong></p></th>
<th class="head"><p><strong>Params</strong></p></th>
<th class="head"><p><strong>GIPS</strong></p></th>
<th class="head"><p><strong>Recipe</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.googlenet.html#torchvision.models.quantization.GoogLeNet_QuantizedWeights" title="torchvision.models.quantization.GoogLeNet_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">GoogLeNet_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>69.826</p></td>
<td><p>89.404</p></td>
<td><p>6.6M</p></td>
<td><p>1.5</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.inception_v3.html#torchvision.models.quantization.Inception_V3_QuantizedWeights" title="torchvision.models.quantization.Inception_V3_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Inception_V3_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>77.176</p></td>
<td><p>93.354</p></td>
<td><p>27.2M</p></td>
<td><p>5.71</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.mobilenet_v2.html#torchvision.models.quantization.MobileNet_V2_QuantizedWeights" title="torchvision.models.quantization.MobileNet_V2_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1</span></code></a></p></td>
<td><p>71.658</p></td>
<td><p>90.15</p></td>
<td><p>3.5M</p></td>
<td><p>0.3</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv2">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.mobilenet_v3_large.html#torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights" title="torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MobileNet_V3_Large_QuantizedWeights.IMAGENET1K_QNNPACK_V1</span></code></a></p></td>
<td><p>73.004</p></td>
<td><p>90.858</p></td>
<td><p>5.5M</p></td>
<td><p>0.22</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#qat-mobilenetv3">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.resnext101_32x8d.html#torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights" title="torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>78.986</p></td>
<td><p>94.48</p></td>
<td><p>88.8M</p></td>
<td><p>16.41</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.resnext101_32x8d.html#torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights" title="torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt101_32X8D_QuantizedWeights.IMAGENET1K_FBGEMM_V2</span></code></a></p></td>
<td><p>82.574</p></td>
<td><p>96.132</p></td>
<td><p>88.8M</p></td>
<td><p>16.41</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.resnext101_64x4d.html#torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights" title="torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNeXt101_64X4D_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>82.898</p></td>
<td><p>96.326</p></td>
<td><p>83.5M</p></td>
<td><p>15.46</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5935">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.resnet18.html#torchvision.models.quantization.ResNet18_QuantizedWeights" title="torchvision.models.quantization.ResNet18_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet18_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>69.494</p></td>
<td><p>88.882</p></td>
<td><p>11.7M</p></td>
<td><p>1.81</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.resnet50.html#torchvision.models.quantization.ResNet50_QuantizedWeights" title="torchvision.models.quantization.ResNet50_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>75.92</p></td>
<td><p>92.814</p></td>
<td><p>25.6M</p></td>
<td><p>4.09</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.resnet50.html#torchvision.models.quantization.ResNet50_QuantizedWeights" title="torchvision.models.quantization.ResNet50_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V2</span></code></a></p></td>
<td><p>80.282</p></td>
<td><p>94.976</p></td>
<td><p>25.6M</p></td>
<td><p>4.09</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.shufflenet_v2_x0_5.html#torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights" title="torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X0_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>57.972</p></td>
<td><p>79.78</p></td>
<td><p>1.4M</p></td>
<td><p>0.04</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.shufflenet_v2_x1_0.html#torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights" title="torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X1_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>68.36</p></td>
<td><p>87.582</p></td>
<td><p>2.3M</p></td>
<td><p>0.14</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/classification#post-training-quantized-models">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.shufflenet_v2_x1_5.html#torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights" title="torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X1_5_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>72.052</p></td>
<td><p>90.7</p></td>
<td><p>3.5M</p></td>
<td><p>0.3</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5906">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.quantization.shufflenet_v2_x2_0.html#torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights" title="torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShuffleNet_V2_X2_0_QuantizedWeights.IMAGENET1K_FBGEMM_V1</span></code></a></p></td>
<td><p>75.354</p></td>
<td><p>92.488</p></td>
<td><p>7.4M</p></td>
<td><p>0.58</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5906">link</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="semantic-segmentation">
<h2>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this heading">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The segmentation module is in Beta stage, and backward compatibility is not guaranteed.</p>
</div>
<p>The following semantic segmentation models are available, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/deeplabv3.html">DeepLabV3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/fcn.html">FCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/lraspp.html">LRASPP</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained semantic segmentation models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io.image</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models.segmentation</span> <span class="kn">import</span> <span class="n">fcn_resnet50</span><span class="p">,</span> <span class="n">FCN_ResNet50_Weights</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms.functional</span> <span class="kn">import</span> <span class="n">to_pil_image</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;gallery/assets/dog1.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">FCN_ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">fcn_resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and visualize the prediction</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)[</span><span class="s2">&quot;out&quot;</span><span class="p">]</span>
<span class="n">normalized_masks</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">class_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="bp">cls</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">])}</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">normalized_masks</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">class_to_idx</span><span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">]]</span>
<span class="n">to_pil_image</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.
The output format of the models is illustrated in <a class="reference internal" href="auto_examples/others/plot_visualization_utils.html#semantic-seg-output"><span class="std std-ref">Semantic segmentation models</span></a>.</p>
<section id="table-of-all-available-semantic-segmentation-weights">
<h3>Table of all available semantic segmentation weights<a class="headerlink" href="#table-of-all-available-semantic-segmentation-weights" title="Permalink to this heading">¶</a></h3>
<p>All models are evaluated a subset of COCO val2017, on the 20 categories that are present in the Pascal VOC dataset:</p>
<table class="table-weights docutils align-default">
<colgroup>
<col style="width: 57%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Weight</strong></p></th>
<th class="head"><p><strong>Mean IoU</strong></p></th>
<th class="head"><p><strong>pixelwise Acc</strong></p></th>
<th class="head"><p><strong>Params</strong></p></th>
<th class="head"><p><strong>GFLOPS</strong></p></th>
<th class="head"><p><strong>Recipe</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.segmentation.deeplabv3_mobilenet_v3_large.html#torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights" title="torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1</span></code></a></p></td>
<td><p>60.3</p></td>
<td><p>91.2</p></td>
<td><p>11.0M</p></td>
<td><p>10.45</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_mobilenet_v3_large">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.segmentation.deeplabv3_resnet101.html#torchvision.models.segmentation.DeepLabV3_ResNet101_Weights" title="torchvision.models.segmentation.DeepLabV3_ResNet101_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1</span></code></a></p></td>
<td><p>67.4</p></td>
<td><p>92.4</p></td>
<td><p>61.0M</p></td>
<td><p>258.74</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet101">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html#torchvision.models.segmentation.DeepLabV3_ResNet50_Weights" title="torchvision.models.segmentation.DeepLabV3_ResNet50_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1</span></code></a></p></td>
<td><p>66.4</p></td>
<td><p>92.4</p></td>
<td><p>42.0M</p></td>
<td><p>178.72</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet50">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.segmentation.fcn_resnet101.html#torchvision.models.segmentation.FCN_ResNet101_Weights" title="torchvision.models.segmentation.FCN_ResNet101_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">FCN_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1</span></code></a></p></td>
<td><p>63.7</p></td>
<td><p>91.9</p></td>
<td><p>54.3M</p></td>
<td><p>232.74</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/segmentation#deeplabv3_resnet101">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.segmentation.fcn_resnet50.html#torchvision.models.segmentation.FCN_ResNet50_Weights" title="torchvision.models.segmentation.FCN_ResNet50_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1</span></code></a></p></td>
<td><p>60.5</p></td>
<td><p>91.4</p></td>
<td><p>35.3M</p></td>
<td><p>152.72</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/segmentation#fcn_resnet50">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.segmentation.lraspp_mobilenet_v3_large.html#torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights" title="torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1</span></code></a></p></td>
<td><p>57.9</p></td>
<td><p>91.2</p></td>
<td><p>3.2M</p></td>
<td><p>2.09</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/segmentation#lraspp_mobilenet_v3_large">link</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="object-detection-instance-segmentation-and-person-keypoint-detection">
<span id="object-det-inst-seg-pers-keypoint-det"></span><h2>Object Detection, Instance Segmentation and Person Keypoint Detection<a class="headerlink" href="#object-detection-instance-segmentation-and-person-keypoint-detection" title="Permalink to this heading">¶</a></h2>
<p>The pre-trained models for detection, instance segmentation and
keypoint detection are initialized with the classification models
in torchvision. The models expect a list of <code class="docutils literal notranslate"><span class="pre">Tensor[C,</span> <span class="pre">H,</span> <span class="pre">W]</span></code>.
Check the constructor of the models for more information.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The detection module is in Beta stage, and backward compatibility is not guaranteed.</p>
</div>
<section id="object-detection">
<h3>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">¶</a></h3>
<p>The following object detection models are available, with or without pre-trained
weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/faster_rcnn.html">Faster R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/fcos.html">FCOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/retinanet.html">RetinaNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/ssd.html">SSD</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/ssdlite.html">SSDlite</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained object detection models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io.image</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models.detection</span> <span class="kn">import</span> <span class="n">fasterrcnn_resnet50_fpn_v2</span><span class="p">,</span> <span class="n">FasterRCNN_ResNet50_FPN_V2_Weights</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">draw_bounding_boxes</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms.functional</span> <span class="kn">import</span> <span class="n">to_pil_image</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;test/assets/encode_jpeg/grace_hopper_517x606.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">FasterRCNN_ResNet50_FPN_V2_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">fasterrcnn_resnet50_fpn_v2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">box_score_thresh</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)]</span>

<span class="c1"># Step 4: Use the model and visualize the prediction</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]]</span>
<span class="n">box</span> <span class="o">=</span> <span class="n">draw_bounding_boxes</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;boxes&quot;</span><span class="p">],</span>
                          <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                          <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>
                          <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">font_size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">to_pil_image</span><span class="p">(</span><span class="n">box</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
<span class="n">im</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.
For details on how to plot the bounding boxes of the models, you may refer to <a class="reference internal" href="auto_examples/others/plot_visualization_utils.html#instance-seg-output"><span class="std std-ref">Instance segmentation models</span></a>.</p>
<section id="table-of-all-available-object-detection-weights">
<h4>Table of all available Object detection weights<a class="headerlink" href="#table-of-all-available-object-detection-weights" title="Permalink to this heading">¶</a></h4>
<p>Box MAPs are reported on COCO val2017:</p>
<table class="table-weights docutils align-default">
<colgroup>
<col style="width: 63%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Weight</strong></p></th>
<th class="head"><p><strong>Box MAP</strong></p></th>
<th class="head"><p><strong>Params</strong></p></th>
<th class="head"><p><strong>GFLOPS</strong></p></th>
<th class="head"><p><strong>Recipe</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.fcos_resnet50_fpn.html#torchvision.models.detection.FCOS_ResNet50_FPN_Weights" title="torchvision.models.detection.FCOS_ResNet50_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">FCOS_ResNet50_FPN_Weights.COCO_V1</span></code></a></p></td>
<td><p>39.2</p></td>
<td><p>32.3M</p></td>
<td><p>128.21</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#fcos-resnet-50-fpn">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn.html#torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights" title="torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.COCO_V1</span></code></a></p></td>
<td><p>22.8</p></td>
<td><p>19.4M</p></td>
<td><p>0.72</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-320-fpn">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn.html#torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights" title="torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1</span></code></a></p></td>
<td><p>32.8</p></td>
<td><p>19.4M</p></td>
<td><p>4.49</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-mobilenetv3-large-fpn">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn_v2.html#torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights" title="torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1</span></code></a></p></td>
<td><p>46.7</p></td>
<td><p>43.7M</p></td>
<td><p>280.37</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5763">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights" title="torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">FasterRCNN_ResNet50_FPN_Weights.COCO_V1</span></code></a></p></td>
<td><p>37</p></td>
<td><p>41.8M</p></td>
<td><p>134.38</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#faster-r-cnn-resnet-50-fpn">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html#torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights" title="torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1</span></code></a></p></td>
<td><p>41.5</p></td>
<td><p>38.2M</p></td>
<td><p>152.24</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5756">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.retinanet_resnet50_fpn.html#torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights" title="torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetinaNet_ResNet50_FPN_Weights.COCO_V1</span></code></a></p></td>
<td><p>36.4</p></td>
<td><p>34.0M</p></td>
<td><p>151.54</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#retinanet">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.ssd300_vgg16.html#torchvision.models.detection.SSD300_VGG16_Weights" title="torchvision.models.detection.SSD300_VGG16_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">SSD300_VGG16_Weights.COCO_V1</span></code></a></p></td>
<td><p>25.1</p></td>
<td><p>35.6M</p></td>
<td><p>34.86</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#ssd300-vgg16">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html#torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights" title="torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">SSDLite320_MobileNet_V3_Large_Weights.COCO_V1</span></code></a></p></td>
<td><p>21.3</p></td>
<td><p>3.4M</p></td>
<td><p>0.58</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#ssdlite320-mobilenetv3-large">link</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="instance-segmentation">
<h3>Instance Segmentation<a class="headerlink" href="#instance-segmentation" title="Permalink to this heading">¶</a></h3>
<p>The following instance segmentation models are available, with or without pre-trained
weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/mask_rcnn.html">Mask R-CNN</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For details on how to plot the masks of the models, you may refer to <a class="reference internal" href="auto_examples/others/plot_visualization_utils.html#instance-seg-output"><span class="std std-ref">Instance segmentation models</span></a>.</p>
<section id="table-of-all-available-instance-segmentation-weights">
<h4>Table of all available Instance segmentation weights<a class="headerlink" href="#table-of-all-available-instance-segmentation-weights" title="Permalink to this heading">¶</a></h4>
<p>Box and Mask MAPs are reported on COCO val2017:</p>
<table class="table-weights docutils align-default">
<colgroup>
<col style="width: 57%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Weight</strong></p></th>
<th class="head"><p><strong>Box MAP</strong></p></th>
<th class="head"><p><strong>Mask MAP</strong></p></th>
<th class="head"><p><strong>Params</strong></p></th>
<th class="head"><p><strong>GFLOPS</strong></p></th>
<th class="head"><p><strong>Recipe</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn_v2.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights" title="torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1</span></code></a></p></td>
<td><p>47.4</p></td>
<td><p>41.8</p></td>
<td><p>46.4M</p></td>
<td><p>333.58</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/pull/5773">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html#torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights" title="torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaskRCNN_ResNet50_FPN_Weights.COCO_V1</span></code></a></p></td>
<td><p>37.9</p></td>
<td><p>34.6</p></td>
<td><p>44.4M</p></td>
<td><p>134.38</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#mask-r-cnn">link</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="keypoint-detection">
<h3>Keypoint Detection<a class="headerlink" href="#keypoint-detection" title="Permalink to this heading">¶</a></h3>
<p>The following person keypoint detection models are available, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/keypoint_rcnn.html">Keypoint R-CNN</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;keypoint_names&quot;]</span></code>.
For details on how to plot the bounding boxes of the models, you may refer to <a class="reference internal" href="auto_examples/others/plot_visualization_utils.html#keypoint-output"><span class="std std-ref">Visualizing keypoints</span></a>.</p>
<section id="table-of-all-available-keypoint-detection-weights">
<h4>Table of all available Keypoint detection weights<a class="headerlink" href="#table-of-all-available-keypoint-detection-weights" title="Permalink to this heading">¶</a></h4>
<p>Box and Keypoint MAPs are reported on COCO val2017:</p>
<table class="table-weights docutils align-default">
<colgroup>
<col style="width: 57%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Weight</strong></p></th>
<th class="head"><p><strong>Box MAP</strong></p></th>
<th class="head"><p><strong>Keypoint MAP</strong></p></th>
<th class="head"><p><strong>Params</strong></p></th>
<th class="head"><p><strong>GFLOPS</strong></p></th>
<th class="head"><p><strong>Recipe</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights" title="torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">KeypointRCNN_ResNet50_FPN_Weights.COCO_LEGACY</span></code></a></p></td>
<td><p>50.6</p></td>
<td><p>61.1</p></td>
<td><p>59.1M</p></td>
<td><p>133.92</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/issues/1606">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights" title="torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">KeypointRCNN_ResNet50_FPN_Weights.COCO_V1</span></code></a></p></td>
<td><p>54.6</p></td>
<td><p>65</p></td>
<td><p>59.1M</p></td>
<td><p>137.42</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/detection#keypoint-r-cnn">link</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="video-classification">
<h2>Video Classification<a class="headerlink" href="#video-classification" title="Permalink to this heading">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The video module is in Beta stage, and backward compatibility is not guaranteed.</p>
</div>
<p>The following video classification models are available, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/video_mvit.html">Video MViT</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/video_resnet.html">Video ResNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/video_s3d.html">Video S3D</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/video_swin_transformer.html">Video SwinTransformer</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained video classification models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io.video</span> <span class="kn">import</span> <span class="n">read_video</span>
<span class="kn">from</span> <span class="nn">torchvision.models.video</span> <span class="kn">import</span> <span class="n">r3d_18</span><span class="p">,</span> <span class="n">R3D_18_Weights</span>

<span class="n">vid</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">read_video</span><span class="p">(</span><span class="s2">&quot;test/assets/videos/v_SoccerJuggling_g23_c01.avi&quot;</span><span class="p">,</span> <span class="n">output_format</span><span class="o">=</span><span class="s2">&quot;TCHW&quot;</span><span class="p">)</span>
<span class="n">vid</span> <span class="o">=</span> <span class="n">vid</span><span class="p">[:</span><span class="mi">32</span><span class="p">]</span>  <span class="c1"># optionally shorten duration</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">R3D_18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">r3d_18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">vid</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">label</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">score</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.</p>
<section id="table-of-all-available-video-classification-weights">
<h3>Table of all available video classification weights<a class="headerlink" href="#table-of-all-available-video-classification-weights" title="Permalink to this heading">¶</a></h3>
<p>Accuracies are reported on Kinetics-400 using single crops for clip length 16:</p>
<table class="table-weights docutils align-default">
<colgroup>
<col style="width: 57%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Weight</strong></p></th>
<th class="head"><p><strong>Acc&#64;1</strong></p></th>
<th class="head"><p><strong>Acc&#64;5</strong></p></th>
<th class="head"><p><strong>Params</strong></p></th>
<th class="head"><p><strong>GFLOPS</strong></p></th>
<th class="head"><p><strong>Recipe</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.mc3_18.html#torchvision.models.video.MC3_18_Weights" title="torchvision.models.video.MC3_18_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MC3_18_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>63.96</p></td>
<td><p>84.13</p></td>
<td><p>11.7M</p></td>
<td><p>43.34</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/video_classification">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.mvit_v1_b.html#torchvision.models.video.MViT_V1_B_Weights" title="torchvision.models.video.MViT_V1_B_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MViT_V1_B_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>78.477</p></td>
<td><p>93.582</p></td>
<td><p>36.6M</p></td>
<td><p>70.6</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/pytorchvideo/blob/main/docs/source/model_zoo.md">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.mvit_v2_s.html#torchvision.models.video.MViT_V2_S_Weights" title="torchvision.models.video.MViT_V2_S_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">MViT_V2_S_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>80.757</p></td>
<td><p>94.665</p></td>
<td><p>34.5M</p></td>
<td><p>64.22</p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/SlowFast/blob/main/MODEL_ZOO.md">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.r2plus1d_18.html#torchvision.models.video.R2Plus1D_18_Weights" title="torchvision.models.video.R2Plus1D_18_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">R2Plus1D_18_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>67.463</p></td>
<td><p>86.175</p></td>
<td><p>31.5M</p></td>
<td><p>40.52</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/video_classification">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.r3d_18.html#torchvision.models.video.R3D_18_Weights" title="torchvision.models.video.R3D_18_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">R3D_18_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>63.2</p></td>
<td><p>83.479</p></td>
<td><p>33.4M</p></td>
<td><p>40.7</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/video_classification">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.s3d.html#torchvision.models.video.S3D_Weights" title="torchvision.models.video.S3D_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">S3D_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>68.368</p></td>
<td><p>88.05</p></td>
<td><p>8.3M</p></td>
<td><p>17.98</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/vision/tree/main/references/video_classification#s3d">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.swin3d_b.html#torchvision.models.video.Swin3D_B_Weights" title="torchvision.models.video.Swin3D_B_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin3D_B_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>79.427</p></td>
<td><p>94.386</p></td>
<td><p>88.0M</p></td>
<td><p>140.67</p></td>
<td><p><a class="reference external" href="https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.swin3d_b.html#torchvision.models.video.Swin3D_B_Weights" title="torchvision.models.video.Swin3D_B_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin3D_B_Weights.KINETICS400_IMAGENET22K_V1</span></code></a></p></td>
<td><p>81.643</p></td>
<td><p>95.574</p></td>
<td><p>88.0M</p></td>
<td><p>140.67</p></td>
<td><p><a class="reference external" href="https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400">link</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.swin3d_s.html#torchvision.models.video.Swin3D_S_Weights" title="torchvision.models.video.Swin3D_S_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin3D_S_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>79.521</p></td>
<td><p>94.158</p></td>
<td><p>49.8M</p></td>
<td><p>82.84</p></td>
<td><p><a class="reference external" href="https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="models/generated/torchvision.models.video.swin3d_t.html#torchvision.models.video.Swin3D_T_Weights" title="torchvision.models.video.Swin3D_T_Weights"><code class="xref py py-class docutils literal notranslate"><span class="pre">Swin3D_T_Weights.KINETICS400_V1</span></code></a></p></td>
<td><p>77.715</p></td>
<td><p>93.519</p></td>
<td><p>28.2M</p></td>
<td><p>43.88</p></td>
<td><p><a class="reference external" href="https://github.com/SwinTransformer/Video-Swin-Transformer#kinetics-400">link</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="optical-flow">
<h2>Optical Flow<a class="headerlink" href="#optical-flow" title="Permalink to this heading">¶</a></h2>
<p>The following Optical Flow models are available, with or without pre-trained</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/raft.html">RAFT</a></li>
</ul>
</div>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torchvision.models.get_model.html" class="btn btn-neutral float-right" title="get_model" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="generated/torchvision.tv_tensors.wrap.html" class="btn btn-neutral" title="wrap" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-present, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Models and pre-trained weights</a><ul>
<li><a class="reference internal" href="#general-information-on-pre-trained-weights">General information on pre-trained weights</a><ul>
<li><a class="reference internal" href="#initializing-pre-trained-models">Initializing pre-trained models</a></li>
<li><a class="reference internal" href="#using-the-pre-trained-models">Using the pre-trained models</a></li>
<li><a class="reference internal" href="#listing-and-retrieving-available-models">Listing and retrieving available models</a></li>
<li><a class="reference internal" href="#using-models-from-hub">Using models from Hub</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification">Classification</a><ul>
<li><a class="reference internal" href="#table-of-all-available-classification-weights">Table of all available classification weights</a></li>
<li><a class="reference internal" href="#quantized-models">Quantized models</a><ul>
<li><a class="reference internal" href="#table-of-all-available-quantized-classification-weights">Table of all available quantized classification weights</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#semantic-segmentation">Semantic Segmentation</a><ul>
<li><a class="reference internal" href="#table-of-all-available-semantic-segmentation-weights">Table of all available semantic segmentation weights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#object-detection-instance-segmentation-and-person-keypoint-detection">Object Detection, Instance Segmentation and Person Keypoint Detection</a><ul>
<li><a class="reference internal" href="#object-detection">Object Detection</a><ul>
<li><a class="reference internal" href="#table-of-all-available-object-detection-weights">Table of all available Object detection weights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#instance-segmentation">Instance Segmentation</a><ul>
<li><a class="reference internal" href="#table-of-all-available-instance-segmentation-weights">Table of all available Instance segmentation weights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#keypoint-detection">Keypoint Detection</a><ul>
<li><a class="reference internal" href="#table-of-all-available-keypoint-detection-weights">Table of all available Keypoint detection weights</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#video-classification">Video Classification</a><ul>
<li><a class="reference internal" href="#table-of-all-available-video-classification-weights">Table of all available video classification weights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optical-flow">Optical Flow</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>