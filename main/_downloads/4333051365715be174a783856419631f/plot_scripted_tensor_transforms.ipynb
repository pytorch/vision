{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tensor transforms and JIT\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [collab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_scripted_tensor_transforms.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_others_plot_scripted_tensor_transforms.py>` to download the full example code.</p></div>\n\nThis example illustrates various features that are now supported by the\n`image transformations <transforms>` on Tensor images. In particular, we\nshow how image transforms can be performed on GPU, and how one can also script\nthem using JIT compilation.\n\nPrior to v0.8.0, transforms in torchvision have traditionally been PIL-centric\nand presented multiple limitations due to that. Now, since v0.8.0, transforms\nimplementations are Tensor and PIL compatible, and we can achieve the following\nnew features:\n\n- transform multi-band torch tensor images (with more than 3-4 channels)\n- torchscript transforms together with your model for deployment\n- support for GPU acceleration\n- batched transformation such as for videos\n- read and decode data directly as torch tensor with torchscript support (for PNG and JPEG image formats)\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>These features are only possible with **Tensor** images.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.io import read_image\n\n\nplt.rcParams[\"savefig.bbox\"] = 'tight'\ntorch.manual_seed(1)\n\n\ndef show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = T.ToPILImage()(img.to('cpu'))\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :func:`~torchvision.io.read_image` function allows to read an image and\ndirectly load it as a tensor\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dog1 = read_image(str(Path('../assets') / 'dog1.jpg'))\ndog2 = read_image(str(Path('../assets') / 'dog2.jpg'))\nshow([dog1, dog2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforming images on GPU\nMost transforms natively support tensors on top of PIL images (to visualize\nthe effect of the transforms, you may refer to see\n`sphx_glr_auto_examples_others_plot_transforms.py`).\nUsing tensor images, we can run the transforms on GPUs if cuda is available!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n\ntransforms = torch.nn.Sequential(\n    T.RandomCrop(224),\n    T.RandomHorizontalFlip(p=0.3),\n)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndog1 = dog1.to(device)\ndog2 = dog2.to(device)\n\ntransformed_dog1 = transforms(dog1)\ntransformed_dog2 = transforms(dog2)\nshow([transformed_dog1, transformed_dog2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scriptable transforms for easier deployment via torchscript\nWe now show how to combine image transformations and a model forward pass,\nwhile using ``torch.jit.script`` to obtain a single scripted module.\n\nLet's define a ``Predictor`` module that transforms the input tensor and then\napplies an ImageNet model on it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n\n\nclass Predictor(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        weights = ResNet18_Weights.DEFAULT\n        self.resnet18 = resnet18(weights=weights, progress=False).eval()\n        self.transforms = weights.transforms()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        with torch.no_grad():\n            x = self.transforms(x)\n            y_pred = self.resnet18(x)\n            return y_pred.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's define scripted and non-scripted instances of ``Predictor`` and\napply it on multiple tensor images of the same size\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictor = Predictor().to(device)\nscripted_predictor = torch.jit.script(predictor).to(device)\n\nbatch = torch.stack([dog1, dog2]).to(device)\n\nres = predictor(batch)\nres_scripted = scripted_predictor(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can verify that the prediction of the scripted and non-scripted models are\nthe same:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\n\nwith open(Path('../assets') / 'imagenet_class_index.json') as labels_file:\n    labels = json.load(labels_file)\n\nfor i, (pred, pred_scripted) in enumerate(zip(res, res_scripted)):\n    assert pred == pred_scripted\n    print(f\"Prediction for Dog {i + 1}: {labels[str(pred.item())]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the model is scripted, it can be easily dumped on disk and re-used\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tempfile\n\nwith tempfile.NamedTemporaryFile() as f:\n    scripted_predictor.save(f.name)\n\n    dumped_scripted_predictor = torch.jit.load(f.name)\n    res_scripted_dumped = dumped_scripted_predictor(batch)\nassert (res_scripted_dumped == res_scripted).all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}