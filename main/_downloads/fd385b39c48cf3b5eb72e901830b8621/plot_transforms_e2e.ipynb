{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Transforms v2: End-to-end object detection/segmentation example\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [Colab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_transforms_e2e.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_transforms_plot_transforms_e2e.py>` to download the full example code.</p></div>\n\nObject detection and segmentation tasks are natively supported:\n``torchvision.transforms.v2`` enables jointly transforming images, videos,\nbounding boxes, and masks.\n\nThis example showcases an end-to-end instance segmentation training case using\nTorchvision utils from ``torchvision.datasets``, ``torchvision.models`` and\n``torchvision.transforms.v2``. Everything covered here can be applied similarly\nto object detection or semantic segmentation tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\n\nimport torch\nimport torch.utils.data\n\nfrom torchvision import models, datasets, tv_tensors\nfrom torchvision.transforms import v2\n\ntorch.manual_seed(0)\n\n# This loads fake data for illustration purposes of this example. In practice, you'll have\n# to replace this with the proper data.\n# If you're trying to run that on Colab, you can download the assets and the\n# helpers from https://github.com/pytorch/vision/tree/main/gallery/\nROOT = pathlib.Path(\"../assets\") / \"coco\"\nIMAGES_PATH = str(ROOT / \"images\")\nANNOTATIONS_PATH = str(ROOT / \"instances.json\")\nfrom helpers import plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset preparation\n\nWe start off by loading the :class:`~torchvision.datasets.CocoDetection` dataset to have a look at what it currently\nreturns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = datasets.CocoDetection(IMAGES_PATH, ANNOTATIONS_PATH)\n\nsample = dataset[0]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Torchvision datasets preserve the data structure and types as it was intended\nby the datasets authors. So by default, the output structure may not always be\ncompatible with the models or the transforms.\n\nTo overcome that, we can use the\n:func:`~torchvision.datasets.wrap_dataset_for_transforms_v2` function. For\n:class:`~torchvision.datasets.CocoDetection`, this changes the target\nstructure to a single dictionary of lists:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=(\"boxes\", \"labels\", \"masks\"))\n\nsample = dataset[0]\nimg, target = sample\nprint(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\nprint(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\\n{type(target['masks']) = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used the ``target_keys`` parameter to specify the kind of output we're\ninterested in. Our dataset now returns a target which is dict where the values\nare `TVTensors <what_are_tv_tensors>` (all are :class:`torch.Tensor`\nsubclasses). We're dropped all unncessary keys from the previous output, but\nif you need any of the original keys e.g. \"image_id\", you can still ask for\nit.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you just want to do detection, you don't need and shouldn't pass\n    \"masks\" in ``target_keys``: if masks are present in the sample, they will\n    be transformed, slowing down your transformations unnecessarily.</p></div>\n\nAs baseline, let's have a look at a sample without transformations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot([dataset[0], dataset[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms\n\nLet's now define our pre-processing transforms. All the transforms know how\nto handle images, bounding boxes and masks when relevant.\n\nTransforms are typically passed as the ``transforms`` parameter of the\ndataset so that they can leverage multi-processing from the\n:class:`torch.utils.data.DataLoader`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transforms = v2.Compose(\n    [\n        v2.ToImage(),\n        v2.RandomPhotometricDistort(p=1),\n        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        v2.RandomIoUCrop(),\n        v2.RandomHorizontalFlip(p=1),\n        v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\ndataset = datasets.CocoDetection(IMAGES_PATH, ANNOTATIONS_PATH, transforms=transforms)\ndataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few things are worth noting here:\n\n- We're converting the PIL image into a\n  :class:`~torchvision.transforms.v2.Image` object. This isn't strictly\n  necessary, but relying on Tensors (here: a Tensor subclass) will\n  `generally be faster <transforms_perf>`.\n- We are calling :class:`~torchvision.transforms.v2.SanitizeBoundingBoxes` to\n  make sure we remove degenerate bounding boxes, as well as their\n  corresponding labels and masks.\n  :class:`~torchvision.transforms.v2.SanitizeBoundingBoxes` should be placed\n  at least once at the end of a detection pipeline; it is particularly\n  critical if :class:`~torchvision.transforms.v2.RandomIoUCrop` was used.\n\nLet's look how the sample looks like with our augmentation pipeline in place:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot([dataset[0], dataset[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the color of the images were distorted, zoomed in or out, and flipped.\nThe bounding boxes and the masks were transformed accordingly. And without any further ado, we can start training.\n\n## Data loading and training loop\n\nBelow we're using Mask-RCNN which is an instance segmentation model, but\neverything we've covered in this tutorial also applies to object detection and\nsemantic segmentation tasks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    # We need a custom collation function here, since the object detection\n    # models expect a sequence of images and target dictionaries. The default\n    # collation function tries to torch.stack() the individual elements,\n    # which fails in general for object detection, because the number of bounding\n    # boxes varies between the images of the same batch.\n    collate_fn=lambda batch: tuple(zip(*batch)),\n)\n\nmodel = models.get_model(\"maskrcnn_resnet50_fpn_v2\", weights=None, weights_backbone=None).train()\n\nfor imgs, targets in data_loader:\n    loss_dict = model(imgs, targets)\n    # Put your training logic here\n\n    print(f\"{[img.shape for img in imgs] = }\")\n    print(f\"{[type(target) for target in targets] = }\")\n    for name, loss_val in loss_dict.items():\n        print(f\"{name:<20}{loss_val:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training References\n\nFrom there, you can check out the [torchvision references](https://github.com/pytorch/vision/tree/main/references) where you'll find\nthe actual training scripts we use to train our models.\n\n**Disclaimer** The code in our references is more complex than what you'll\nneed for your own use-cases: this is because we're supporting different\nbackends (PIL, tensors, TVTensors) and different transforms namespaces (v1 and\nv2). So don't be afraid to simplify and only keep what you need.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}