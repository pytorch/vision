{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to write your own Datapoint class\n\nThis guide is intended for advanced users and downstream library maintainers. We explain how to\nwrite your own datapoint class, and how to make it compatible with the built-in\nTorchvision v2 transforms. Before continuing, make sure you have read\n`sphx_glr_auto_examples_v2_transforms_plot_datapoints.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision\n\n# We are using BETA APIs, so we deactivate the associated warning, thereby acknowledging that\n# some APIs may slightly change in the future\ntorchvision.disable_beta_transforms_warning()\n\nfrom torchvision import datapoints\nfrom torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create a very simple class that just inherits from the base\n:class:`~torchvision.datapoints.Datapoint` class. It will be enough to cover\nwhat you need to know to implement your more elaborate uses-cases. If you need\nto create a class that carries meta-data, take a look at how the\n:class:`~torchvision.datapoints.BoundingBoxes` class is [implemented](https://github.com/pytorch/vision/blob/main/torchvision/datapoints/_bounding_box.py).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyDatapoint(datapoints.Datapoint):\n    pass\n\n\nmy_dp = MyDatapoint([1, 2, 3])\nmy_dp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have defined our custom Datapoint class, we want it to be\ncompatible with the built-in torchvision transforms, and the functional API.\nFor that, we need to implement a kernel which performs the core of the\ntransformation, and then \"hook\" it to the functional that we want to support\nvia :func:`~torchvision.transforms.v2.functional.register_kernel`.\n\nWe illustrate this process below: we create a kernel for the \"horizontal flip\"\noperation of our MyDatapoint class, and register it to the functional API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.v2 import functional as F\n\n\n@F.register_kernel(functional=\"hflip\", datapoint_cls=MyDatapoint)\ndef hflip_my_datapoint(my_dp, *args, **kwargs):\n    print(\"Flipping!\")\n    out = my_dp.flip(-1)\n    return datapoints.wrap(out, like=my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To understand why :func:`~torchvision.datapoints.wrap` is used, see\n`datapoint_unwrapping_behaviour`. Ignore the ``*args, **kwargs`` for now,\nwe will explain it below in `param_forwarding`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In our call to ``register_kernel`` above we used a string\n    ``functional=\"hflip\"`` to refer to the functional we want to hook into. We\n    could also have used the  functional *itself*, i.e.\n    ``@register_kernel(functional=F.hflip, ...)``.</p></div>\n\nNow that we have registered our kernel, we can call the functional API on a\n``MyDatapoint`` instance:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_dp = MyDatapoint(torch.rand(3, 256, 256))\n_ = F.hflip(my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can also use the\n:class:`~torchvision.transforms.v2.RandomHorizontalFlip` transform, since it relies on :func:`~torchvision.transforms.v2.functional.hflip` internally:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t = v2.RandomHorizontalFlip(p=1)\n_ = t(my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>We cannot register a kernel for a transform class, we can only register a\n    kernel for a **functional**. The reason we can't register a transform\n    class is because one transform may internally rely on more than one\n    functional, so in general we can't register a single kernel for a given\n    class.</p></div>\n\n\n## Parameter forwarding, and ensuring future compatibility of your kernels\n\nThe functional API that you're hooking into is public and therefore\n**backward** compatible: we guarantee that the parameters of these functionals\nwon't be removed or renamed without a proper deprecation cycle. However, we\ndon't guarantee **forward** compatibility, and we may add new parameters in\nthe future.\n\nImagine that in a future version, Torchvision adds a new ``inplace`` parameter\nto its :func:`~torchvision.transforms.v2.functional.hflip` functional. If you\nalready defined and registered your own kernel as\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def hflip_my_datapoint(my_dp):  # noqa\n    print(\"Flipping!\")\n    out = my_dp.flip(-1)\n    return datapoints.wrap(out, like=my_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "then calling ``F.hflip(my_dp)`` will **fail**, because ``hflip`` will try to\npass the new ``inplace`` parameter to your kernel, but your kernel doesn't\naccept it.\n\nFor this reason, we recommend to always define your kernels with\n``*args, **kwargs`` in their signature, as done above. This way, your kernel\nwill be able to accept any new parameter that we may add in the future.\n(Technically, adding `**kwargs` only should be enough).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}