{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to write your own v2 transforms\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [collab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_custom_transforms.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_v2_transforms_plot_custom_transforms.py>` to download the full example code.</p></div>\n\nThis guide explains how to write transforms that are compatible with the\ntorchvision transforms V2 API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchvision import datapoints\nfrom torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Just create a ``nn.Module`` and override the ``forward`` method\n\nIn most cases, this is all you're going to need, as long as you already know\nthe structure of the input that your transform will expect. For example if\nyou're just doing image classification, your transform will typically accept a\nsingle image as input, or a ``(img, label)`` input. So you can just hard-code\nyour ``forward`` method to accept just that, e.g.\n\n.. code:: python\n\n    class MyCustomTransform(torch.nn.Module):\n        def forward(self, img, label):\n            # Do some transformations\n            return new_img, new_label\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This means that if you have a custom transform that is already compatible\n    with the V1 transforms (those in ``torchvision.transforms``), it will\n    still work with the V2 transforms without any change!</p></div>\n\nWe will illustrate this more completely below with a typical detection case,\nwhere our samples are just images, bounding boxes and labels:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyCustomTransform(torch.nn.Module):\n    def forward(self, img, bboxes, label):  # we assume inputs are always structured like this\n        print(\n            f\"I'm transforming an image of shape {img.shape} \"\n            f\"with bboxes = {bboxes}\\n{label = }\"\n        )\n        # Do some transformations. Here, we're just passing though the input\n        return img, bboxes, label\n\n\ntransforms = v2.Compose([\n    MyCustomTransform(),\n    v2.RandomResizedCrop((224, 224), antialias=True),\n    v2.RandomHorizontalFlip(p=1),\n    v2.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n])\n\nH, W = 256, 256\nimg = torch.rand(3, H, W)\nbboxes = datapoints.BoundingBoxes(\n    torch.tensor([[0, 10, 10, 20], [50, 50, 70, 70]]),\n    format=\"XYXY\",\n    canvas_size=(H, W)\n)\nlabel = 3\n\nout_img, out_bboxes, out_label = transforms(img, bboxes, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Output image shape: {out_img.shape}\\nout_bboxes = {out_bboxes}\\n{out_label = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>While working with datapoint classes in your code, make sure to\n    familiarize yourself with this section:\n    `datapoint_unwrapping_behaviour`</p></div>\n\n## Supporting arbitrary input structures\n\nIn the section above, we have assumed that you already know the structure of\nyour inputs and that you're OK with hard-coding this expected structure in\nyour code. If you want your custom transforms to be as flexible as possible,\nthis can be a bit limitting.\n\nA key feature of the builtin Torchvision V2 transforms is that they can accept\narbitrary input structure and return the same structure as output (with\ntransformed entries). For example, transforms can accept a single image, or a\ntuple of ``(img, label)``, or an arbitrary nested dictionary as input:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "structured_input = {\n    \"img\": img,\n    \"annotations\": (bboxes, label),\n    \"something_that_will_be_ignored\": (1, \"hello\")\n}\nstructured_output = v2.RandomHorizontalFlip(p=1)(structured_input)\n\nassert isinstance(structured_output, dict)\nassert structured_output[\"something_that_will_be_ignored\"] == (1, \"hello\")\nprint(f\"The transformed bboxes are:\\n{structured_output['annotations'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to reproduce this behavior in your own transform, we invite you to\nlook at our [code](https://github.com/pytorch/vision/blob/main/torchvision/transforms/v2/_transform.py)\nand adapt it to your needs.\n\nIn brief, the core logic is to unpack the input into a flat list using [pytree](https://github.com/pytorch/pytorch/blob/main/torch/utils/_pytree.py), and\nthen transform only the entries that can be transformed (the decision is made\nbased on the **class** of the entries, as all datapoints are\ntensor-subclasses) plus some custom logic that is out of score here - check the\ncode for details. The (potentially transformed) entries are then repacked and\nreturned, in the same structure as the input.\n\nWe do not provide public dev-facing tools to achieve that at this time, but if\nthis is something that would be valuable to you, please let us know by opening\nan issue on our [GitHub repo](https://github.com/pytorch/vision/issues).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}