{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Transforms on KeyPoints\n\nThis example illustrates how to define and use keypoints.\nFor this tutorial, we use this picture of a ceramic figure from the pre-columbian period.\nThe image is specified \"public domain\" (https://www.metmuseum.org/art/collection/search/502727).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Support for keypoints was released in TorchVision 0.23 and is\n    currently a BETA feature. We don't expect the API to change, but there may\n    be some rare edge-cases. If you find any issues, please report them on\n    our bug tracker: https://github.com/pytorch/vision/issues?q=is:open+is:issue</p></div>\n\nFirst, a bit of setup code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n\nimport torch\nfrom torchvision.tv_tensors import KeyPoints\nfrom torchvision.transforms import v2\nfrom helpers import plot\n\nplt.rcParams[\"figure.figsize\"] = [10, 5]\nplt.rcParams[\"savefig.bbox\"] = \"tight\"\n\n# if you change the seed, make sure that the transformed output\n# still make sense\ntorch.manual_seed(0)\n\n# If you're trying to run that on Colab, you can download the assets and the\n# helpers from https://github.com/pytorch/vision/tree/main/gallery/\norig_img = Image.open(Path('../assets') / 'pottery.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating KeyPoints\nKey points are created by instantiating the\n:class:`~torchvision.tv_tensors.KeyPoints` class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "orig_pts = KeyPoints(\n    [\n        [\n            [445, 700],  # nose\n            [320, 660],\n            [370, 660],\n            [420, 660],  # left eye\n            [300, 620],\n            [420, 620],  # left eyebrow\n            [475, 665],\n            [515, 665],\n            [555, 655],  # right eye\n            [460, 625],\n            [560, 600],  # right eyebrow\n            [370, 780],\n            [450, 760],\n            [540, 780],\n            [450, 820],  # mouth\n        ],\n    ],\n    canvas_size=(orig_img.size[1], orig_img.size[0]),\n)\n\nplot([(orig_img, orig_pts)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms illustrations\n\nUsing :class:`~torchvision.transforms.RandomRotation`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rotater = v2.RandomRotation(degrees=(0, 180), expand=True)\nrotated_imgs = [rotater((orig_img, orig_pts)) for _ in range(4)]\nplot([(orig_img, orig_pts)] + rotated_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using :class:`~torchvision.transforms.Pad`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "padded_imgs_and_points = [\n    v2.Pad(padding=padding)(orig_img, orig_pts)\n    for padding in (30, 50, 100, 200)\n]\nplot([(orig_img, orig_pts)] + padded_imgs_and_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using :class:`~torchvision.transforms.Resize`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resized_imgs = [\n    v2.Resize(size=size)(orig_img, orig_pts)\n    for size in (300, 500, 1000, orig_img.size)\n]\nplot([(orig_img, orig_pts)] + resized_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using :class:`~torchvision.transforms.RandomPerspective`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perspective_transformer = v2.RandomPerspective(distortion_scale=0.6, p=1.0)\nperspective_imgs = [perspective_transformer(orig_img, orig_pts) for _ in range(4)]\nplot([(orig_img, orig_pts)] + perspective_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using :class:`~torchvision.transforms.CenterCrop`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "center_crops_and_points = [\n    v2.CenterCrop(size=size)(orig_img, orig_pts)\n    for size in (300, 500, 1000, orig_img.size)\n]\nplot([(orig_img, orig_pts)] + center_crops_and_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using :class:`~torchvision.transforms.RandomRotation`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rotater = v2.RandomRotation(degrees=(0, 180))\nrotated_imgs = [rotater((orig_img, orig_pts)) for _ in range(4)]\nplot([(orig_img, orig_pts)] + rotated_imgs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}