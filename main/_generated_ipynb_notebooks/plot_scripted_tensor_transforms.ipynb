{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Torchscript support\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [Colab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_scripted_tensor_transforms.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_others_plot_scripted_tensor_transforms.py>` to download the full example code.</p></div>\n\nThis example illustrates [torchscript](https://pytorch.org/docs/stable/jit.html) support of the torchvision\n`transforms <transforms>` on Tensor images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nimport torchvision.transforms as v1\nfrom torchvision.io import read_image\n\nplt.rcParams[\"savefig.bbox\"] = 'tight'\ntorch.manual_seed(1)\n\n# If you're trying to run that on Colab, you can download the assets and the\n# helpers from https://github.com/pytorch/vision/tree/main/gallery/\nimport sys\nsys.path += [\"../transforms\"]\nfrom helpers import plot\nASSETS_PATH = Path('../assets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most transforms support torchscript. For composing transforms, we use\n:class:`torch.nn.Sequential` instead of\n:class:`~torchvision.transforms.v2.Compose`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dog1 = read_image(str(ASSETS_PATH / 'dog1.jpg'))\ndog2 = read_image(str(ASSETS_PATH / 'dog2.jpg'))\n\ntransforms = torch.nn.Sequential(\n    v1.RandomCrop(224),\n    v1.RandomHorizontalFlip(p=0.3),\n)\n\nscripted_transforms = torch.jit.script(transforms)\n\nplot([dog1, scripted_transforms(dog1), dog2, scripted_transforms(dog2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Above we have used transforms from the ``torchvision.transforms``\n    namespace, i.e. the \"v1\" transforms. The v2 transforms from the\n    ``torchvision.transforms.v2`` namespace are the `recommended\n    <v1_or_v2>` way to use transforms in your code.\n\n    The v2 transforms also support torchscript, but if you call\n    ``torch.jit.script()`` on a v2 **class** transform, you'll actually end up\n    with its (scripted) v1 equivalent.  This may lead to slightly different\n    results between the scripted and eager executions due to implementation\n    differences between v1 and v2.\n\n    If you really need torchscript support for the v2 transforms, **we\n    recommend scripting the functionals** from the\n    ``torchvision.transforms.v2.functional`` namespace to avoid surprises.</p></div>\n\nBelow we now show how to combine image transformations and a model forward\npass, while using ``torch.jit.script`` to obtain a single scripted module.\n\nLet's define a ``Predictor`` module that transforms the input tensor and then\napplies an ImageNet model on it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n\n\nclass Predictor(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        weights = ResNet18_Weights.DEFAULT\n        self.resnet18 = resnet18(weights=weights, progress=False).eval()\n        self.transforms = weights.transforms(antialias=True)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        with torch.no_grad():\n            x = self.transforms(x)\n            y_pred = self.resnet18(x)\n            return y_pred.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's define scripted and non-scripted instances of ``Predictor`` and\napply it on multiple tensor images of the same size\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npredictor = Predictor().to(device)\nscripted_predictor = torch.jit.script(predictor).to(device)\n\nbatch = torch.stack([dog1, dog2]).to(device)\n\nres = predictor(batch)\nres_scripted = scripted_predictor(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can verify that the prediction of the scripted and non-scripted models are\nthe same:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\n\nwith open(Path('../assets') / 'imagenet_class_index.json') as labels_file:\n    labels = json.load(labels_file)\n\nfor i, (pred, pred_scripted) in enumerate(zip(res, res_scripted)):\n    assert pred == pred_scripted\n    print(f\"Prediction for Dog {i + 1}: {labels[str(pred.item())]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the model is scripted, it can be easily dumped on disk and re-used\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tempfile\n\nwith tempfile.NamedTemporaryFile() as f:\n    scripted_predictor.save(f.name)\n\n    dumped_scripted_predictor = torch.jit.load(f.name)\n    res_scripted_dumped = dumped_scripted_predictor(batch)\nassert (res_scripted_dumped == res_scripted).all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}