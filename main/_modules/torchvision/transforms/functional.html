


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchvision.transforms.functional &mdash; Torchvision main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom_torchvision.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/vision/versions.html'>main (0.25.0a0+faffd5c) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../transforms.html">Transforming images, videos, boxes and more</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tv_tensors.html">TVTensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html">Models and pre-trained weights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../utils.html">Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ops.html">Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../io.html">Decoding / Encoding images and videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_extraction.html">Feature extraction for model inspection</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples and training references</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html">Examples and tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../training_references.html">Training references</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/docs">PyTorch</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torchvision.html">torchvision</a> &gt;</li>
        
      <li>torchvision.transforms.functional</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchvision.transforms.functional</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numbers</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL.Image</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span> <span class="k">as</span> <span class="n">PILImage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">accimage</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">accimage</span> <span class="o">=</span> <span class="kc">None</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">..utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_Image_fromarray</span><span class="p">,</span> <span class="n">_log_api_usage_once</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">_functional_pil</span> <span class="k">as</span> <span class="n">F_pil</span><span class="p">,</span> <span class="n">_functional_tensor</span> <span class="k">as</span> <span class="n">F_t</span>


<span class="k">class</span><span class="w"> </span><span class="nc">InterpolationMode</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Interpolation modes</span>
<span class="sd">    Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``,</span>
<span class="sd">    and ``lanczos``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">NEAREST</span> <span class="o">=</span> <span class="s2">&quot;nearest&quot;</span>
    <span class="n">NEAREST_EXACT</span> <span class="o">=</span> <span class="s2">&quot;nearest-exact&quot;</span>
    <span class="n">BILINEAR</span> <span class="o">=</span> <span class="s2">&quot;bilinear&quot;</span>
    <span class="n">BICUBIC</span> <span class="o">=</span> <span class="s2">&quot;bicubic&quot;</span>
    <span class="c1"># For PIL compatibility</span>
    <span class="n">BOX</span> <span class="o">=</span> <span class="s2">&quot;box&quot;</span>
    <span class="n">HAMMING</span> <span class="o">=</span> <span class="s2">&quot;hamming&quot;</span>
    <span class="n">LANCZOS</span> <span class="o">=</span> <span class="s2">&quot;lanczos&quot;</span>


<span class="c1"># TODO: Once torchscript supports Enums with staticmethod</span>
<span class="c1"># this can be put into InterpolationMode as staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_interpolation_modes_from_int</span><span class="p">(</span><span class="n">i</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InterpolationMode</span><span class="p">:</span>
    <span class="n">inverse_modes_mapping</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">NEAREST</span><span class="p">,</span>
        <span class="mi">2</span><span class="p">:</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BICUBIC</span><span class="p">,</span>
        <span class="mi">4</span><span class="p">:</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BOX</span><span class="p">,</span>
        <span class="mi">5</span><span class="p">:</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">HAMMING</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">LANCZOS</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">inverse_modes_mapping</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>


<span class="n">pil_modes_mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">NEAREST</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BICUBIC</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">NEAREST_EXACT</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BOX</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">HAMMING</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">LANCZOS</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_is_pil_image</span> <span class="o">=</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">_is_pil_image</span>


<div class="viewcode-block" id="get_dimensions"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.get_dimensions.html#torchvision.transforms.functional.get_dimensions">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the dimensions of an image as [channels, height, width].</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): The image to be checked.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: The image dimensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">get_dimensions</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_image_size"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.get_image_size.html#torchvision.transforms.functional.get_image_size">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_image_size</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the size of an image as [width, height].</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): The image to be checked.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: The image size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">get_image_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">get_image_size</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">get_image_size</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_image_num_channels"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.get_image_num_channels.html#torchvision.transforms.functional.get_image_num_channels">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_image_num_channels</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the number of channels of an image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): The image to be checked.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: The number of channels.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">get_image_num_channels</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">get_image_num_channels</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">get_image_num_channels</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">unused</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_is_numpy</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">unused</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_is_numpy_image</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">in</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}</span>


<div class="viewcode-block" id="to_tensor"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.to_tensor.html#torchvision.transforms.functional.to_tensor">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">to_tensor</span><span class="p">(</span><span class="n">pic</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PILImage</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.</span>
<span class="sd">    This function does not support torchscript.</span>

<span class="sd">    See :class:`~torchvision.transforms.ToTensor` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Converted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">F_pil</span><span class="o">.</span><span class="n">_is_pil_image</span><span class="p">(</span><span class="n">pic</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_numpy</span><span class="p">(</span><span class="n">pic</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pic should be PIL Image or ndarray. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">pic</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_is_numpy</span><span class="p">(</span><span class="n">pic</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_numpy_image</span><span class="p">(</span><span class="n">pic</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pic should be 2/3 dimensional. Got </span><span class="si">{</span><span class="n">pic</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> dimensions.&quot;</span><span class="p">)</span>

    <span class="n">default_float_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="c1"># handle numpy array</span>
        <span class="k">if</span> <span class="n">pic</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">pic</span> <span class="o">=</span> <span class="n">pic</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pic</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># backward compatibility</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">default_float_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">img</span>

    <span class="k">if</span> <span class="n">accimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">accimage</span><span class="o">.</span><span class="n">Image</span><span class="p">):</span>
        <span class="n">nppic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">pic</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">pic</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">pic</span><span class="o">.</span><span class="n">width</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">pic</span><span class="o">.</span><span class="n">copyto</span><span class="p">(</span><span class="n">nppic</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">nppic</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">default_float_dtype</span><span class="p">)</span>

    <span class="c1"># handle PIL Image</span>
    <span class="n">mode_to_nptype</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;I&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="s2">&quot;I;16&quot;</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span> <span class="o">==</span> <span class="s2">&quot;little&quot;</span> <span class="k">else</span> <span class="s2">&quot;I;16B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="s2">&quot;F&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">}</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">mode_to_nptype</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pic</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">pic</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;1&quot;</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="mi">255</span> <span class="o">*</span> <span class="n">img</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">pic</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pic</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">get_image_num_channels</span><span class="p">(</span><span class="n">pic</span><span class="p">))</span>
    <span class="c1"># put it from HWC to CHW format</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">default_float_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">img</span></div>


<div class="viewcode-block" id="pil_to_tensor"><a class="viewcode-back" href="../../../generated/torchvision.transforms.v2.functional.pil_to_tensor.html#torchvision.transforms.functional.pil_to_tensor">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">pil_to_tensor</span><span class="p">(</span><span class="n">pic</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a ``PIL Image`` to a tensor of the same type.</span>
<span class="sd">    This function does not support torchscript.</span>

<span class="sd">    See :class:`~torchvision.transforms.PILToTensor` for more details.</span>

<span class="sd">    .. note::</span>

<span class="sd">        A deep copy of the underlying array is performed.</span>

<span class="sd">    Args:</span>
<span class="sd">        pic (PIL Image): Image to be converted to tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Converted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">pil_to_tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">_is_pil_image</span><span class="p">(</span><span class="n">pic</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pic should be PIL Image. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">pic</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">accimage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">accimage</span><span class="o">.</span><span class="n">Image</span><span class="p">):</span>
        <span class="c1"># accimage format is always uint8 internally, so always return uint8 here</span>
        <span class="n">nppic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">pic</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">pic</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">pic</span><span class="o">.</span><span class="n">width</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
        <span class="n">pic</span><span class="o">.</span><span class="n">copyto</span><span class="p">(</span><span class="n">nppic</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">nppic</span><span class="p">)</span>

    <span class="c1"># handle PIL Image</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">pic</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pic</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">get_image_num_channels</span><span class="p">(</span><span class="n">pic</span><span class="p">))</span>
    <span class="c1"># put it from HWC to CHW format</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">img</span></div>


<div class="viewcode-block" id="convert_image_dtype"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.convert_image_dtype.html#torchvision.transforms.functional.convert_image_dtype">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">convert_image_dtype</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a tensor image to the given ``dtype`` and scale the values accordingly</span>
<span class="sd">    This function does not support PIL Image.</span>

<span class="sd">    Args:</span>
<span class="sd">        image (torch.Tensor): Image to be converted</span>
<span class="sd">        dtype (torch.dtype): Desired data type of the output</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Converted image</span>

<span class="sd">    .. note::</span>

<span class="sd">        When converting from a smaller to a larger integer ``dtype`` the maximum values are **not** mapped exactly.</span>
<span class="sd">        If converted back and forth, this mismatch has no effect.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: When trying to cast :class:`torch.float32` to :class:`torch.int32` or :class:`torch.int64` as</span>
<span class="sd">            well as for trying to cast :class:`torch.float64` to :class:`torch.int64`. These conversions might lead to</span>
<span class="sd">            overflow errors since the floating point ``dtype`` cannot store consecutive integers over the whole range</span>
<span class="sd">            of the integer ``dtype``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">convert_image_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input img should be Tensor Image&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">convert_image_dtype</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="to_pil_image"><a class="viewcode-back" href="../../../generated/torchvision.transforms.v2.functional.to_pil_image.html#torchvision.transforms.functional.to_pil_image">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">to_pil_image</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert a tensor or an ndarray to PIL Image. This function does not support torchscript.</span>

<span class="sd">    See :class:`~torchvision.transforms.ToPILImage` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.</span>
<span class="sd">        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).</span>

<span class="sd">    .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image: Image converted to PIL Image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">to_pil_image</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">pic</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">pic</span> <span class="o">=</span> <span class="n">pic</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">pic</span> <span class="o">=</span> <span class="n">pic</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pic should be Tensor or ndarray. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">pic</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pic</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># if 2D image, add channel dimension (HWC)</span>
        <span class="n">pic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">pic</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pic</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pic should be 2/3 dimensional. Got </span><span class="si">{</span><span class="n">pic</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> dimensions.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pic</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pic should not have &gt; 4 channels. Got </span><span class="si">{</span><span class="n">pic</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> channels.&quot;</span><span class="p">)</span>

    <span class="n">npimg</span> <span class="o">=</span> <span class="n">pic</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floating</span><span class="p">)</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;F&quot;</span><span class="p">:</span>
        <span class="n">npimg</span> <span class="o">=</span> <span class="p">(</span><span class="n">npimg</span> <span class="o">*</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">npimg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">expected_mode</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">npimg</span> <span class="o">=</span> <span class="n">npimg</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="n">expected_mode</span> <span class="o">=</span> <span class="s2">&quot;L&quot;</span>
        <span class="k">elif</span> <span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span>
            <span class="n">expected_mode</span> <span class="o">=</span> <span class="s2">&quot;I;16&quot;</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span> <span class="o">==</span> <span class="s2">&quot;little&quot;</span> <span class="k">else</span> <span class="s2">&quot;I;16B&quot;</span>
        <span class="k">elif</span> <span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="n">expected_mode</span> <span class="o">=</span> <span class="s2">&quot;I&quot;</span>
        <span class="k">elif</span> <span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">expected_mode</span> <span class="o">=</span> <span class="s2">&quot;F&quot;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="n">expected_mode</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Incorrect mode (</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">) supplied for input type </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">. Should be </span><span class="si">{</span><span class="n">expected_mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">expected_mode</span>

    <span class="k">elif</span> <span class="n">npimg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">permitted_2_channel_modes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;LA&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">permitted_2_channel_modes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only modes </span><span class="si">{</span><span class="n">permitted_2_channel_modes</span><span class="si">}</span><span class="s2"> are supported for 2D inputs&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;LA&quot;</span>

    <span class="k">elif</span> <span class="n">npimg</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">permitted_4_channel_modes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;RGBA&quot;</span><span class="p">,</span> <span class="s2">&quot;CMYK&quot;</span><span class="p">,</span> <span class="s2">&quot;RGBX&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">permitted_4_channel_modes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only modes </span><span class="si">{</span><span class="n">permitted_4_channel_modes</span><span class="si">}</span><span class="s2"> are supported for 4D inputs&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;RGBA&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">permitted_3_channel_modes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;RGB&quot;</span><span class="p">,</span> <span class="s2">&quot;YCbCr&quot;</span><span class="p">,</span> <span class="s2">&quot;HSV&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">permitted_3_channel_modes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Only modes </span><span class="si">{</span><span class="n">permitted_3_channel_modes</span><span class="si">}</span><span class="s2"> are supported for 3D inputs&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;RGB&quot;</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input type </span><span class="si">{</span><span class="n">npimg</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> is not supported&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_Image_fromarray</span><span class="p">(</span><span class="n">npimg</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="normalize"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.normalize.html#torchvision.transforms.functional.normalize">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">normalize</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">std</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Normalize a float tensor image with mean and standard deviation.</span>
<span class="sd">    This transform does not support PIL Image.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This transform acts out of place by default, i.e., it does not mutates the input tensor.</span>

<span class="sd">    See :class:`~torchvision.transforms.Normalize` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Float tensor image of size (C, H, W) or (B, C, H, W) to be normalized.</span>
<span class="sd">        mean (sequence): Sequence of means for each channel.</span>
<span class="sd">        std (sequence): Sequence of standard deviations for each channel.</span>
<span class="sd">        inplace(bool,optional): Bool to make this operation inplace.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Normalized Tensor image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;img should be Tensor Image. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_compute_resized_output_size</span><span class="p">(</span>
    <span class="n">image_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">max_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">allow_size_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># only True in v2</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">image_size</span>
    <span class="n">short</span><span class="p">,</span> <span class="n">long</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="o">&lt;=</span> <span class="n">h</span> <span class="k">else</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_size_none</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This should never happen!!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;max_size must be an integer when size is None, but got </span><span class="si">{</span><span class="n">max_size</span><span class="si">}</span><span class="s2"> instead.&quot;</span><span class="p">)</span>
        <span class="n">new_short</span><span class="p">,</span> <span class="n">new_long</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">max_size</span> <span class="o">*</span> <span class="n">short</span> <span class="o">/</span> <span class="n">long</span><span class="p">),</span> <span class="n">max_size</span>
        <span class="n">new_w</span><span class="p">,</span> <span class="n">new_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_short</span><span class="p">,</span> <span class="n">new_long</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="o">&lt;=</span> <span class="n">h</span> <span class="k">else</span> <span class="p">(</span><span class="n">new_long</span><span class="p">,</span> <span class="n">new_short</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># specified size only for the smallest edge</span>
        <span class="n">requested_new_short</span> <span class="o">=</span> <span class="n">size</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">new_short</span><span class="p">,</span> <span class="n">new_long</span> <span class="o">=</span> <span class="n">requested_new_short</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">requested_new_short</span> <span class="o">*</span> <span class="n">long</span> <span class="o">/</span> <span class="n">short</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">max_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">max_size</span> <span class="o">&lt;=</span> <span class="n">requested_new_short</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;max_size = </span><span class="si">{</span><span class="n">max_size</span><span class="si">}</span><span class="s2"> must be strictly greater than the requested &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;size for the smaller edge size = </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">new_long</span> <span class="o">&gt;</span> <span class="n">max_size</span><span class="p">:</span>
                <span class="n">new_short</span><span class="p">,</span> <span class="n">new_long</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">max_size</span> <span class="o">*</span> <span class="n">new_short</span> <span class="o">/</span> <span class="n">new_long</span><span class="p">),</span> <span class="n">max_size</span>

        <span class="n">new_w</span><span class="p">,</span> <span class="n">new_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_short</span><span class="p">,</span> <span class="n">new_long</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="o">&lt;=</span> <span class="n">h</span> <span class="k">else</span> <span class="p">(</span><span class="n">new_long</span><span class="p">,</span> <span class="n">new_short</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># specified both h and w</span>
        <span class="n">new_w</span><span class="p">,</span> <span class="n">new_h</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span><span class="p">]</span>


<div class="viewcode-block" id="resize"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.resize.html#torchvision.transforms.functional.resize">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">resize</span><span class="p">(</span>
    <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">interpolation</span><span class="p">:</span> <span class="n">InterpolationMode</span> <span class="o">=</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
    <span class="n">max_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">antialias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resize the input image to the given size.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be resized.</span>
<span class="sd">        size (sequence or int): Desired output size. If size is a sequence like</span>
<span class="sd">            (h, w), the output size will be matched to this. If size is an int,</span>
<span class="sd">            the smaller edge of the image will be matched to this number maintaining</span>
<span class="sd">            the aspect ratio. i.e, if height &gt; width, then image will be rescaled to</span>
<span class="sd">            :math:`\left(\text{size} \times \frac{\text{height}}{\text{width}}, \text{size}\right)`.</span>

<span class="sd">            .. note::</span>
<span class="sd">                In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.</span>
<span class="sd">        interpolation (InterpolationMode): Desired interpolation enum defined by</span>
<span class="sd">            :class:`torchvision.transforms.InterpolationMode`.</span>
<span class="sd">            Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``,</span>
<span class="sd">            ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are</span>
<span class="sd">            supported.</span>
<span class="sd">            The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well.</span>
<span class="sd">        max_size (int, optional): The maximum allowed for the longer edge of</span>
<span class="sd">            the resized image. If the longer edge of the image is greater</span>
<span class="sd">            than ``max_size`` after being resized according to ``size``,</span>
<span class="sd">            ``size`` will be overruled so that the longer edge is equal to</span>
<span class="sd">            ``max_size``.</span>
<span class="sd">            As a result, the smaller edge may be shorter than ``size``. This</span>
<span class="sd">            is only supported if ``size`` is an int (or a sequence of length</span>
<span class="sd">            1 in torchscript mode).</span>
<span class="sd">        antialias (bool, optional): Whether to apply antialiasing.</span>
<span class="sd">            It only affects **tensors** with bilinear or bicubic modes and it is</span>
<span class="sd">            ignored otherwise: on PIL images, antialiasing is always applied on</span>
<span class="sd">            bilinear or bicubic modes; on other modes (for PIL images and</span>
<span class="sd">            tensors), antialiasing makes no sense and this parameter is ignored.</span>
<span class="sd">            Possible values are:</span>

<span class="sd">            - ``True`` (default): will apply antialiasing for bilinear or bicubic modes.</span>
<span class="sd">              Other mode aren&#39;t affected. This is probably what you want to use.</span>
<span class="sd">            - ``False``: will not apply antialiasing for tensors on any mode. PIL</span>
<span class="sd">              images are still antialiased on bilinear or bicubic modes, because</span>
<span class="sd">              PIL doesn&#39;t support no antialias.</span>
<span class="sd">            - ``None``: equivalent to ``False`` for tensors and ``True`` for</span>
<span class="sd">              PIL images. This value exists for legacy reasons and you probably</span>
<span class="sd">              don&#39;t want to use it unless you really know what you are doing.</span>

<span class="sd">            The default value changed from ``None`` to ``True`` in</span>
<span class="sd">            v0.17, for the PIL and Tensor backends to be consistent.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Resized image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">resize</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">interpolation</span> <span class="o">=</span> <span class="n">_interpolation_modes_from_int</span><span class="p">(</span><span class="n">interpolation</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="n">InterpolationMode</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;Argument interpolation should be a InterpolationMode or a corresponding Pillow integer constant&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Size must be an int or a 1 or 2 element tuple/list, not a </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="si">}</span><span class="s2"> element tuple/list&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">max_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;max_size should only be passed if size specifies the length of the smaller edge, &quot;</span>
                <span class="s2">&quot;i.e. size should be an int or a sequence of length 1 in torchscript mode.&quot;</span>
            <span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">=</span> <span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="n">size</span><span class="p">]</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_compute_resized_output_size</span><span class="p">((</span><span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span><span class="p">),</span> <span class="n">size</span><span class="p">,</span> <span class="n">max_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">[</span><span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span><span class="p">]</span> <span class="o">==</span> <span class="n">output_size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">img</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">antialias</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.&quot;</span><span class="p">)</span>
        <span class="n">pil_interpolation</span> <span class="o">=</span> <span class="n">pil_modes_mapping</span><span class="p">[</span><span class="n">interpolation</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">pil_interpolation</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interpolation</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">antialias</span><span class="o">=</span><span class="n">antialias</span><span class="p">)</span></div>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.pad.html#torchvision.transforms.functional.pad">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">pad</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">padding</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">fill</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;constant&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pad the given image on all sides with the given &quot;pad&quot; value.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means at most 2 leading dimensions for mode reflect and symmetric,</span>
<span class="sd">    at most 3 leading dimensions for mode edge,</span>
<span class="sd">    and an arbitrary number of leading dimensions for mode constant</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be padded.</span>
<span class="sd">        padding (int or sequence): Padding on each border. If a single int is provided this</span>
<span class="sd">            is used to pad all borders. If sequence of length 2 is provided this is the padding</span>
<span class="sd">            on left/right and top/bottom respectively. If a sequence of length 4 is provided</span>
<span class="sd">            this is the padding for the left, top, right and bottom borders respectively.</span>

<span class="sd">            .. note::</span>
<span class="sd">                In torchscript mode padding as single int is not supported, use a sequence of</span>
<span class="sd">                length 1: ``[padding, ]``.</span>
<span class="sd">        fill (number or tuple): Pixel fill value for constant fill. Default is 0.</span>
<span class="sd">            If a tuple of length 3, it is used to fill R, G, B channels respectively.</span>
<span class="sd">            This value is only used when the padding_mode is constant.</span>
<span class="sd">            Only number is supported for torch Tensor.</span>
<span class="sd">            Only int or tuple value is supported for PIL Image.</span>
<span class="sd">        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric.</span>
<span class="sd">            Default is constant.</span>

<span class="sd">            - constant: pads with a constant value, this value is specified with fill</span>

<span class="sd">            - edge: pads with the last value at the edge of the image.</span>
<span class="sd">              If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2</span>

<span class="sd">            - reflect: pads with reflection of image without repeating the last value on the edge.</span>
<span class="sd">              For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode</span>
<span class="sd">              will result in [3, 2, 1, 2, 3, 4, 3, 2]</span>

<span class="sd">            - symmetric: pads with reflection of image repeating the last value on the edge.</span>
<span class="sd">              For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode</span>
<span class="sd">              will result in [2, 1, 1, 2, 3, 4, 4, 3]</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Padded image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="crop"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.crop.html#torchvision.transforms.functional.crop">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">crop</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">top</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">left</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Crop the given image at specified location and output size.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.</span>
<span class="sd">    If image size is smaller than output size along any edge, image is padded with 0 and then cropped.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be cropped. (0,0) denotes the top left corner of the image.</span>
<span class="sd">        top (int): Vertical component of the top left corner of the crop box.</span>
<span class="sd">        left (int): Horizontal component of the top left corner of the crop box.</span>
<span class="sd">        height (int): Height of the crop box.</span>
<span class="sd">        width (int): Width of the crop box.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Cropped image.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">crop</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span></div>


<div class="viewcode-block" id="center_crop"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.center_crop.html#torchvision.transforms.functional.center_crop">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">center_crop</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Crops the given image at the center.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.</span>
<span class="sd">    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be cropped.</span>
<span class="sd">        output_size (sequence or int): (height, width) of the crop box. If int or sequence with single int,</span>
<span class="sd">            it is used for both directions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Cropped image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">center_crop</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">output_size</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">=</span> <span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span> <span class="o">=</span> <span class="n">output_size</span>

    <span class="k">if</span> <span class="n">crop_width</span> <span class="o">&gt;</span> <span class="n">image_width</span> <span class="ow">or</span> <span class="n">crop_height</span> <span class="o">&gt;</span> <span class="n">image_height</span><span class="p">:</span>
        <span class="n">padding_ltrb</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">crop_width</span> <span class="o">-</span> <span class="n">image_width</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">crop_width</span> <span class="o">&gt;</span> <span class="n">image_width</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="p">(</span><span class="n">crop_height</span> <span class="o">-</span> <span class="n">image_height</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">crop_height</span> <span class="o">&gt;</span> <span class="n">image_height</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="p">(</span><span class="n">crop_width</span> <span class="o">-</span> <span class="n">image_width</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">crop_width</span> <span class="o">&gt;</span> <span class="n">image_width</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="p">(</span><span class="n">crop_height</span> <span class="o">-</span> <span class="n">image_height</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">crop_height</span> <span class="o">&gt;</span> <span class="n">image_height</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">padding_ltrb</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># PIL uses fill value 0</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">=</span> <span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">crop_width</span> <span class="o">==</span> <span class="n">image_width</span> <span class="ow">and</span> <span class="n">crop_height</span> <span class="o">==</span> <span class="n">image_height</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">img</span>

    <span class="n">crop_top</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">((</span><span class="n">image_height</span> <span class="o">-</span> <span class="n">crop_height</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">))</span>
    <span class="n">crop_left</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">((</span><span class="n">image_width</span> <span class="o">-</span> <span class="n">crop_width</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">crop_top</span><span class="p">,</span> <span class="n">crop_left</span><span class="p">,</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">)</span></div>


<div class="viewcode-block" id="resized_crop"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.resized_crop.html#torchvision.transforms.functional.resized_crop">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">resized_crop</span><span class="p">(</span>
    <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">top</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">left</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">interpolation</span><span class="p">:</span> <span class="n">InterpolationMode</span> <span class="o">=</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
    <span class="n">antialias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Crop the given image and resize it to desired size.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions</span>

<span class="sd">    Notably used in :class:`~torchvision.transforms.RandomResizedCrop`.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be cropped. (0,0) denotes the top left corner of the image.</span>
<span class="sd">        top (int): Vertical component of the top left corner of the crop box.</span>
<span class="sd">        left (int): Horizontal component of the top left corner of the crop box.</span>
<span class="sd">        height (int): Height of the crop box.</span>
<span class="sd">        width (int): Width of the crop box.</span>
<span class="sd">        size (sequence or int): Desired output size. Same semantics as ``resize``.</span>
<span class="sd">        interpolation (InterpolationMode): Desired interpolation enum defined by</span>
<span class="sd">            :class:`torchvision.transforms.InterpolationMode`.</span>
<span class="sd">            Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``,</span>
<span class="sd">            ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are</span>
<span class="sd">            supported.</span>
<span class="sd">            The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well.</span>
<span class="sd">        antialias (bool, optional): Whether to apply antialiasing.</span>
<span class="sd">            It only affects **tensors** with bilinear or bicubic modes and it is</span>
<span class="sd">            ignored otherwise: on PIL images, antialiasing is always applied on</span>
<span class="sd">            bilinear or bicubic modes; on other modes (for PIL images and</span>
<span class="sd">            tensors), antialiasing makes no sense and this parameter is ignored.</span>
<span class="sd">            Possible values are:</span>

<span class="sd">            - ``True`` (default): will apply antialiasing for bilinear or bicubic modes.</span>
<span class="sd">              Other mode aren&#39;t affected. This is probably what you want to use.</span>
<span class="sd">            - ``False``: will not apply antialiasing for tensors on any mode. PIL</span>
<span class="sd">              images are still antialiased on bilinear or bicubic modes, because</span>
<span class="sd">              PIL doesn&#39;t support no antialias.</span>
<span class="sd">            - ``None``: equivalent to ``False`` for tensors and ``True`` for</span>
<span class="sd">              PIL images. This value exists for legacy reasons and you probably</span>
<span class="sd">              don&#39;t want to use it unless you really know what you are doing.</span>

<span class="sd">            The default value changed from ``None`` to ``True`` in</span>
<span class="sd">            v0.17, for the PIL and Tensor backends to be consistent.</span>
<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Cropped image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">resized_crop</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">interpolation</span><span class="p">,</span> <span class="n">antialias</span><span class="o">=</span><span class="n">antialias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span></div>


<div class="viewcode-block" id="hflip"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.hflip.html#torchvision.transforms.functional.hflip">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">hflip</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Horizontally flip the given image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be flipped. If img</span>
<span class="sd">            is a Tensor, it is expected to be in [..., H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading</span>
<span class="sd">            dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor:  Horizontally flipped image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">hflip</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">hflip</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">hflip</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_perspective_coeffs</span><span class="p">(</span><span class="n">startpoints</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">endpoints</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to get the coefficients (a, b, c, d, e, f, g, h) for the perspective transforms.</span>

<span class="sd">    In Perspective Transform each pixel (x, y) in the original image gets transformed as,</span>
<span class="sd">     (x, y) -&gt; ( (ax + by + c) / (gx + hy + 1), (dx + ey + f) / (gx + hy + 1) )</span>

<span class="sd">    Args:</span>
<span class="sd">        startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners</span>
<span class="sd">            ``[top-left, top-right, bottom-right, bottom-left]`` of the original image.</span>
<span class="sd">        endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners</span>
<span class="sd">            ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image.</span>

<span class="sd">    Returns:</span>
<span class="sd">        octuple (a, b, c, d, e, f, g, h) for transforming each pixel.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">startpoints</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">endpoints</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Please provide exactly four corners, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">startpoints</span><span class="p">)</span><span class="si">}</span><span class="s2"> startpoints and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">endpoints</span><span class="p">)</span><span class="si">}</span><span class="s2"> endpoints.&quot;</span>
        <span class="p">)</span>
    <span class="n">a_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">startpoints</span><span class="p">),</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">endpoints</span><span class="p">,</span> <span class="n">startpoints</span><span class="p">)):</span>
        <span class="n">a_matrix</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">a_matrix</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

    <span class="n">b_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">startpoints</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
    <span class="c1"># do least squares in double precision to prevent numerical issues</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">a_matrix</span><span class="p">,</span> <span class="n">b_matrix</span><span class="p">,</span> <span class="n">driver</span><span class="o">=</span><span class="s2">&quot;gels&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">solution</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">output</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span>


<div class="viewcode-block" id="perspective"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.perspective.html#torchvision.transforms.functional.perspective">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">perspective</span><span class="p">(</span>
    <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">startpoints</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">endpoints</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">interpolation</span><span class="p">:</span> <span class="n">InterpolationMode</span> <span class="o">=</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
    <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform perspective transform of the given image.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be transformed.</span>
<span class="sd">        startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners</span>
<span class="sd">            ``[top-left, top-right, bottom-right, bottom-left]`` of the original image.</span>
<span class="sd">        endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners</span>
<span class="sd">            ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image.</span>
<span class="sd">        interpolation (InterpolationMode): Desired interpolation enum defined by</span>
<span class="sd">            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.</span>
<span class="sd">            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.</span>
<span class="sd">            The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well.</span>
<span class="sd">        fill (sequence or number, optional): Pixel fill value for the area outside the transformed</span>
<span class="sd">            image. If given a number, the value is used for all bands respectively.</span>

<span class="sd">            .. note::</span>
<span class="sd">                In torchscript mode single int/float value is not supported, please use a sequence</span>
<span class="sd">                of length 1: ``[value, ]``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: transformed Image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">perspective</span><span class="p">)</span>

    <span class="n">coeffs</span> <span class="o">=</span> <span class="n">_get_perspective_coeffs</span><span class="p">(</span><span class="n">startpoints</span><span class="p">,</span> <span class="n">endpoints</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">interpolation</span> <span class="o">=</span> <span class="n">_interpolation_modes_from_int</span><span class="p">(</span><span class="n">interpolation</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="n">InterpolationMode</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;Argument interpolation should be a InterpolationMode or a corresponding Pillow integer constant&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">pil_interpolation</span> <span class="o">=</span> <span class="n">pil_modes_mapping</span><span class="p">[</span><span class="n">interpolation</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">perspective</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">pil_interpolation</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">perspective</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interpolation</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">)</span></div>


<div class="viewcode-block" id="vflip"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.vflip.html#torchvision.transforms.functional.vflip">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">vflip</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Vertically flip the given image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be flipped. If img</span>
<span class="sd">            is a Tensor, it is expected to be in [..., H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading</span>
<span class="sd">            dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor:  Vertically flipped image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">vflip</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">vflip</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">vflip</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<div class="viewcode-block" id="five_crop"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.five_crop.html#torchvision.transforms.functional.five_crop">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">five_crop</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Crop the given image into four corners and the central crop.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions</span>

<span class="sd">    .. Note::</span>
<span class="sd">        This transform returns a tuple of images and there may be a</span>
<span class="sd">        mismatch in the number of inputs and targets your ``Dataset`` returns.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be cropped.</span>
<span class="sd">        size (sequence or int): Desired output size of the crop. If size is an</span>
<span class="sd">            int instead of sequence like (h, w), a square crop (size, size) is</span>
<span class="sd">            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).</span>

<span class="sd">    Returns:</span>
<span class="sd">       tuple: tuple (tl, tr, bl, br, center)</span>
<span class="sd">       Corresponding top left, top right, bottom left, bottom right and center crop.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">five_crop</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please provide only two dimensions (h, w) for size.&quot;</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">=</span> <span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span> <span class="o">=</span> <span class="n">size</span>
    <span class="k">if</span> <span class="n">crop_width</span> <span class="o">&gt;</span> <span class="n">image_width</span> <span class="ow">or</span> <span class="n">crop_height</span> <span class="o">&gt;</span> <span class="n">image_height</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Requested crop size </span><span class="si">{}</span><span class="s2"> is bigger than input size </span><span class="si">{}</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span><span class="p">)))</span>

    <span class="n">tl</span> <span class="o">=</span> <span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">)</span>
    <span class="n">tr</span> <span class="o">=</span> <span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">-</span> <span class="n">crop_width</span><span class="p">,</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">)</span>
    <span class="n">bl</span> <span class="o">=</span> <span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">image_height</span> <span class="o">-</span> <span class="n">crop_height</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">)</span>
    <span class="n">br</span> <span class="o">=</span> <span class="n">crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">image_height</span> <span class="o">-</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">image_width</span> <span class="o">-</span> <span class="n">crop_width</span><span class="p">,</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">)</span>

    <span class="n">center</span> <span class="o">=</span> <span class="n">center_crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">tl</span><span class="p">,</span> <span class="n">tr</span><span class="p">,</span> <span class="n">bl</span><span class="p">,</span> <span class="n">br</span><span class="p">,</span> <span class="n">center</span></div>


<div class="viewcode-block" id="ten_crop"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.ten_crop.html#torchvision.transforms.functional.ten_crop">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">ten_crop</span><span class="p">(</span>
    <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">vertical_flip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate ten cropped images from the given image.</span>
<span class="sd">    Crop the given image into four corners and the central crop plus the</span>
<span class="sd">    flipped version of these (horizontal flipping is used by default).</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions</span>

<span class="sd">    .. Note::</span>
<span class="sd">        This transform returns a tuple of images and there may be a</span>
<span class="sd">        mismatch in the number of inputs and targets your ``Dataset`` returns.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be cropped.</span>
<span class="sd">        size (sequence or int): Desired output size of the crop. If size is an</span>
<span class="sd">            int instead of sequence like (h, w), a square crop (size, size) is</span>
<span class="sd">            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).</span>
<span class="sd">        vertical_flip (bool): Use vertical flipping instead of horizontal</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: tuple (tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip)</span>
<span class="sd">        Corresponding top left, top right, bottom left, bottom right and</span>
<span class="sd">        center crop and same for the flipped image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">ten_crop</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please provide only two dimensions (h, w) for size.&quot;</span><span class="p">)</span>

    <span class="n">first_five</span> <span class="o">=</span> <span class="n">five_crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">vertical_flip</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">vflip</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">hflip</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="n">second_five</span> <span class="o">=</span> <span class="n">five_crop</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">first_five</span> <span class="o">+</span> <span class="n">second_five</span></div>


<div class="viewcode-block" id="adjust_brightness"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.adjust_brightness.html#torchvision.transforms.functional.adjust_brightness">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">adjust_brightness</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">brightness_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adjust brightness of an image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be adjusted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">        brightness_factor (float):  How much to adjust the brightness. Can be</span>
<span class="sd">            any non-negative number. 0 gives a black image, 1 gives the</span>
<span class="sd">            original image while 2 increases the brightness by a factor of 2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Brightness adjusted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">adjust_brightness</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">adjust_brightness</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">brightness_factor</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">adjust_brightness</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">brightness_factor</span><span class="p">)</span></div>


<div class="viewcode-block" id="adjust_contrast"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.adjust_contrast.html#torchvision.transforms.functional.adjust_contrast">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">adjust_contrast</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">contrast_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adjust contrast of an image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be adjusted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">        contrast_factor (float): How much to adjust the contrast. Can be any</span>
<span class="sd">            non-negative number. 0 gives a solid gray image, 1 gives the</span>
<span class="sd">            original image while 2 increases the contrast by a factor of 2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Contrast adjusted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">adjust_contrast</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">adjust_contrast</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">contrast_factor</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">adjust_contrast</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">contrast_factor</span><span class="p">)</span></div>


<div class="viewcode-block" id="adjust_saturation"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.adjust_saturation.html#torchvision.transforms.functional.adjust_saturation">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">adjust_saturation</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">saturation_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adjust color saturation of an image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be adjusted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">        saturation_factor (float):  How much to adjust the saturation. 0 will</span>
<span class="sd">            give a black and white image, 1 will give the original image while</span>
<span class="sd">            2 will enhance the saturation by a factor of 2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Saturation adjusted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">adjust_saturation</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">adjust_saturation</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">saturation_factor</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">adjust_saturation</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">saturation_factor</span><span class="p">)</span></div>


<div class="viewcode-block" id="adjust_hue"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.adjust_hue.html#torchvision.transforms.functional.adjust_hue">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">adjust_hue</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">hue_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adjust hue of an image.</span>

<span class="sd">    The image hue is adjusted by converting the image to HSV and</span>
<span class="sd">    cyclically shifting the intensities in the hue channel (H).</span>
<span class="sd">    The image is then converted back to original image mode.</span>

<span class="sd">    `hue_factor` is the amount of shift in H channel and must be in the</span>
<span class="sd">    interval `[-0.5, 0.5]`.</span>

<span class="sd">    See `Hue`_ for more details.</span>

<span class="sd">    .. _Hue: https://en.wikipedia.org/wiki/Hue</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be adjusted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            If img is PIL Image mode &quot;1&quot;, &quot;I&quot;, &quot;F&quot; and modes with transparency (alpha channel) are not supported.</span>
<span class="sd">            Note: the pixel values of the input image has to be non-negative for conversion to HSV space;</span>
<span class="sd">            thus it does not work if you normalize your image to an interval with negative values,</span>
<span class="sd">            or use an interpolation that generates negative values before using this function.</span>
<span class="sd">        hue_factor (float):  How much to shift the hue channel. Should be in</span>
<span class="sd">            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in</span>
<span class="sd">            HSV space in positive and negative direction respectively.</span>
<span class="sd">            0 means no shift. Therefore, both -0.5 and 0.5 will give an image</span>
<span class="sd">            with complementary colors while 0 gives the original image.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Hue adjusted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">adjust_hue</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">adjust_hue</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">hue_factor</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">adjust_hue</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">hue_factor</span><span class="p">)</span></div>


<div class="viewcode-block" id="adjust_gamma"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.adjust_gamma.html#torchvision.transforms.functional.adjust_gamma">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">adjust_gamma</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">gain</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Perform gamma correction on an image.</span>

<span class="sd">    Also known as Power Law Transform. Intensities in RGB mode are adjusted</span>
<span class="sd">    based on the following equation:</span>

<span class="sd">    .. math::</span>
<span class="sd">        I_{\text{out}} = 255 \times \text{gain} \times \left(\frac{I_{\text{in}}}{255}\right)^{\gamma}</span>

<span class="sd">    See `Gamma Correction`_ for more details.</span>

<span class="sd">    .. _Gamma Correction: https://en.wikipedia.org/wiki/Gamma_correction</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): PIL Image to be adjusted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            If img is PIL Image, modes with transparency (alpha channel) are not supported.</span>
<span class="sd">        gamma (float): Non negative real number, same as :math:`\gamma` in the equation.</span>
<span class="sd">            gamma larger than 1 make the shadows darker,</span>
<span class="sd">            while gamma smaller than 1 make dark regions lighter.</span>
<span class="sd">        gain (float): The constant multiplier.</span>
<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Gamma correction adjusted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">adjust_gamma</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">adjust_gamma</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gain</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">adjust_gamma</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gain</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_inverse_affine_matrix</span><span class="p">(</span>
    <span class="n">center</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">angle</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">translate</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">shear</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">inverted</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="c1"># Helper method to compute inverse matrix for affine transformation</span>

    <span class="c1"># Pillow requires inverse affine transformation matrix:</span>
    <span class="c1"># Affine matrix is : M = T * C * RotateScaleShear * C^-1</span>
    <span class="c1">#</span>
    <span class="c1"># where T is translation matrix: [1, 0, tx | 0, 1, ty | 0, 0, 1]</span>
    <span class="c1">#       C is translation matrix to keep center: [1, 0, cx | 0, 1, cy | 0, 0, 1]</span>
    <span class="c1">#       RotateScaleShear is rotation with scale and shear matrix</span>
    <span class="c1">#</span>
    <span class="c1">#       RotateScaleShear(a, s, (sx, sy)) =</span>
    <span class="c1">#       = R(a) * S(s) * SHy(sy) * SHx(sx)</span>
    <span class="c1">#       = [ s*cos(a - sy)/cos(sy), s*(-cos(a - sy)*tan(sx)/cos(sy) - sin(a)), 0 ]</span>
    <span class="c1">#         [ s*sin(a - sy)/cos(sy), s*(-sin(a - sy)*tan(sx)/cos(sy) + cos(a)), 0 ]</span>
    <span class="c1">#         [ 0                    , 0                                      , 1 ]</span>
    <span class="c1"># where R is a rotation matrix, S is a scaling matrix, and SHx and SHy are the shears:</span>
    <span class="c1"># SHx(s) = [1, -tan(s)] and SHy(s) = [1      , 0]</span>
    <span class="c1">#          [0, 1      ]              [-tan(s), 1]</span>
    <span class="c1">#</span>
    <span class="c1"># Thus, the inverse is M^-1 = C * RotateScaleShear^-1 * C^-1 * T^-1</span>

    <span class="n">rot</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>
    <span class="n">sx</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">shear</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">sy</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">shear</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">cx</span><span class="p">,</span> <span class="n">cy</span> <span class="o">=</span> <span class="n">center</span>
    <span class="n">tx</span><span class="p">,</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">translate</span>

    <span class="c1"># RSS without scaling</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">rot</span> <span class="o">-</span> <span class="n">sy</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">sy</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">rot</span> <span class="o">-</span> <span class="n">sy</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">sx</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">sy</span><span class="p">)</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">rot</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">rot</span> <span class="o">-</span> <span class="n">sy</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">sy</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">rot</span> <span class="o">-</span> <span class="n">sy</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">sx</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">sy</span><span class="p">)</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">rot</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">inverted</span><span class="p">:</span>
        <span class="c1"># Inverted rotation matrix with scale and shear</span>
        <span class="c1"># det([[a, b], [c, d]]) == 1, since det(rotation) = 1 and det(shear) = 1</span>
        <span class="n">matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">,</span> <span class="o">-</span><span class="n">b</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="n">c</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
        <span class="n">matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">/</span> <span class="n">scale</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">matrix</span><span class="p">]</span>
        <span class="c1"># Apply inverse of translation and of center translation: RSS^-1 * C^-1 * T^-1</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cx</span> <span class="o">-</span> <span class="n">tx</span><span class="p">)</span> <span class="o">+</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cy</span> <span class="o">-</span> <span class="n">ty</span><span class="p">)</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cx</span> <span class="o">-</span> <span class="n">tx</span><span class="p">)</span> <span class="o">+</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cy</span> <span class="o">-</span> <span class="n">ty</span><span class="p">)</span>
        <span class="c1"># Apply center translation: C * RSS^-1 * C^-1 * T^-1</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">cx</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">+=</span> <span class="n">cy</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
        <span class="n">matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">*</span> <span class="n">scale</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">matrix</span><span class="p">]</span>
        <span class="c1"># Apply inverse of center translation: RSS * C^-1</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cy</span><span class="p">)</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">cy</span><span class="p">)</span>
        <span class="c1"># Apply translation and center : T * C * RSS * C^-1</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">cx</span> <span class="o">+</span> <span class="n">tx</span>
        <span class="n">matrix</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">+=</span> <span class="n">cy</span> <span class="o">+</span> <span class="n">ty</span>

    <span class="k">return</span> <span class="n">matrix</span>


<div class="viewcode-block" id="rotate"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.rotate.html#torchvision.transforms.functional.rotate">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">rotate</span><span class="p">(</span>
    <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">angle</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">interpolation</span><span class="p">:</span> <span class="n">InterpolationMode</span> <span class="o">=</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">NEAREST</span><span class="p">,</span>
    <span class="n">expand</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">center</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rotate the image by angle.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): image to be rotated.</span>
<span class="sd">        angle (number): rotation angle value in degrees, counter-clockwise.</span>
<span class="sd">        interpolation (InterpolationMode): Desired interpolation enum defined by</span>
<span class="sd">            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.</span>
<span class="sd">            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.</span>
<span class="sd">            The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well.</span>
<span class="sd">        expand (bool, optional): Optional expansion flag.</span>
<span class="sd">            If true, expands the output image to make it large enough to hold the entire rotated image.</span>
<span class="sd">            If false or omitted, make the output image the same size as the input image.</span>
<span class="sd">            Note that the expand flag assumes rotation around the center and no translation.</span>
<span class="sd">        center (sequence, optional): Optional center of rotation. Origin is the upper left corner.</span>
<span class="sd">            Default is the center of the image.</span>
<span class="sd">        fill (sequence or number, optional): Pixel fill value for the area outside the transformed</span>
<span class="sd">            image. If given a number, the value is used for all bands respectively.</span>

<span class="sd">            .. note::</span>
<span class="sd">                In torchscript mode single int/float value is not supported, please use a sequence</span>
<span class="sd">                of length 1: ``[value, ]``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Rotated image.</span>

<span class="sd">    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">rotate</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">interpolation</span> <span class="o">=</span> <span class="n">_interpolation_modes_from_int</span><span class="p">(</span><span class="n">interpolation</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="n">InterpolationMode</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;Argument interpolation should be a InterpolationMode or a corresponding Pillow integer constant&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">angle</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument angle should be int or float&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">center</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument center should be a sequence&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">pil_interpolation</span> <span class="o">=</span> <span class="n">pil_modes_mapping</span><span class="p">[</span><span class="n">interpolation</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="n">angle</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">pil_interpolation</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="n">expand</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">)</span>

    <span class="n">center_f</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">center</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        <span class="c1"># Center values should be in pixel coordinates but translated such that (0, 0) corresponds to image center.</span>
        <span class="n">center_f</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">c</span> <span class="o">-</span> <span class="n">s</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="p">[</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">])]</span>

    <span class="c1"># due to current incoherence of rotation angle direction between affine and rotate implementations</span>
    <span class="c1"># we need to set -angle.</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">_get_inverse_affine_matrix</span><span class="p">(</span><span class="n">center_f</span><span class="p">,</span> <span class="o">-</span><span class="n">angle</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interpolation</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="n">expand</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">)</span></div>


<div class="viewcode-block" id="affine"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.affine.html#torchvision.transforms.functional.affine">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">affine</span><span class="p">(</span>
    <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">angle</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">translate</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">shear</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">interpolation</span><span class="p">:</span> <span class="n">InterpolationMode</span> <span class="o">=</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">NEAREST</span><span class="p">,</span>
    <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">center</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply affine transformation on the image keeping image center invariant.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): image to transform.</span>
<span class="sd">        angle (number): rotation angle in degrees between -180 and 180, clockwise direction.</span>
<span class="sd">        translate (sequence of integers): horizontal and vertical translations (post-rotation translation)</span>
<span class="sd">        scale (float): overall scale</span>
<span class="sd">        shear (float or sequence): shear angle value in degrees between -180 to 180, clockwise direction.</span>
<span class="sd">            If a sequence is specified, the first value corresponds to a shear parallel to the x-axis, while</span>
<span class="sd">            the second value corresponds to a shear parallel to the y-axis.</span>
<span class="sd">        interpolation (InterpolationMode): Desired interpolation enum defined by</span>
<span class="sd">            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.</span>
<span class="sd">            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.</span>
<span class="sd">            The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well.</span>
<span class="sd">        fill (sequence or number, optional): Pixel fill value for the area outside the transformed</span>
<span class="sd">            image. If given a number, the value is used for all bands respectively.</span>

<span class="sd">            .. note::</span>
<span class="sd">                In torchscript mode single int/float value is not supported, please use a sequence</span>
<span class="sd">                of length 1: ``[value, ]``.</span>
<span class="sd">        center (sequence, optional): Optional center of rotation. Origin is the upper left corner.</span>
<span class="sd">            Default is the center of the image.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Transformed image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">affine</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">interpolation</span> <span class="o">=</span> <span class="n">_interpolation_modes_from_int</span><span class="p">(</span><span class="n">interpolation</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="n">InterpolationMode</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;Argument interpolation should be a InterpolationMode or a corresponding Pillow integer constant&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">angle</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument angle should be int or float&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">translate</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument translate should be a sequence&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">translate</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument translate should be a sequence of length 2&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">scale</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument scale should be positive&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shear</span><span class="p">,</span> <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Shear should be either a single value or a sequence of two values&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">angle</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">translate</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">translate</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">translate</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shear</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="n">shear</span> <span class="o">=</span> <span class="p">[</span><span class="n">shear</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shear</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">shear</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">shear</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shear</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">shear</span> <span class="o">=</span> <span class="p">[</span><span class="n">shear</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shear</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shear</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shear should be a sequence containing two values. Got </span><span class="si">{</span><span class="n">shear</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">center</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument center should be a sequence&quot;</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># center = (width * 0.5 + 0.5, height * 0.5 + 0.5)</span>
        <span class="c1"># it is visually better to estimate the center without 0.5 offset</span>
        <span class="c1"># otherwise image rotated by 90 degrees is shifted vs output image of torch.rot90 or F_t.affine</span>
        <span class="k">if</span> <span class="n">center</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">center</span> <span class="o">=</span> <span class="p">[</span><span class="n">width</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">]</span>
        <span class="n">matrix</span> <span class="o">=</span> <span class="n">_get_inverse_affine_matrix</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">angle</span><span class="p">,</span> <span class="n">translate</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">shear</span><span class="p">)</span>
        <span class="n">pil_interpolation</span> <span class="o">=</span> <span class="n">pil_modes_mapping</span><span class="p">[</span><span class="n">interpolation</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">pil_interpolation</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">)</span>

    <span class="n">center_f</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">center</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">get_dimensions</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        <span class="c1"># Center values should be in pixel coordinates but translated such that (0, 0) corresponds to image center.</span>
        <span class="n">center_f</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">c</span> <span class="o">-</span> <span class="n">s</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="p">[</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">])]</span>

    <span class="n">translate_f</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">translate</span><span class="p">]</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">_get_inverse_affine_matrix</span><span class="p">(</span><span class="n">center_f</span><span class="p">,</span> <span class="n">angle</span><span class="p">,</span> <span class="n">translate_f</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">shear</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">matrix</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interpolation</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">)</span></div>


<span class="c1"># Looks like to_grayscale() is a stand-alone functional that is never called</span>
<span class="c1"># from the transform classes. Perhaps it&#39;s still here for BC? I can&#39;t be</span>
<span class="c1"># bothered to dig.</span>
<div class="viewcode-block" id="to_grayscale"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.to_grayscale.html#torchvision.transforms.functional.to_grayscale">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">unused</span>
<span class="k">def</span><span class="w"> </span><span class="nf">to_grayscale</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">num_output_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert PIL image of any mode (RGB, HSV, LAB, etc) to grayscale version of image.</span>
<span class="sd">    This transform does not support torch Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image): PIL Image to be converted to grayscale.</span>
<span class="sd">        num_output_channels (int): number of channels of the output image. Value can be 1 or 3. Default is 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image: Grayscale version of the image.</span>

<span class="sd">        - if num_output_channels = 1 : returned image is single channel</span>
<span class="sd">        - if num_output_channels = 3 : returned image is 3 channel with r = g = b</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">to_grayscale</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">Image</span><span class="o">.</span><span class="n">Image</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">to_grayscale</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">num_output_channels</span><span class="p">)</span>

    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input should be PIL Image&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="rgb_to_grayscale"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.rgb_to_grayscale.html#torchvision.transforms.functional.rgb_to_grayscale">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">rgb_to_grayscale</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">num_output_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert RGB image to grayscale version of image.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., 3, H, W] shape, where ... means an arbitrary number of leading dimensions</span>

<span class="sd">    Note:</span>
<span class="sd">        Please, note that this method supports only RGB images as input. For inputs in other color spaces,</span>
<span class="sd">        please, consider using :meth:`~torchvision.transforms.functional.to_grayscale` with PIL Image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): RGB Image to be converted to grayscale.</span>
<span class="sd">        num_output_channels (int): number of channels of the output image. Value can be 1 or 3. Default, 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Grayscale version of the image.</span>

<span class="sd">        - if num_output_channels = 1 : returned image is single channel</span>
<span class="sd">        - if num_output_channels = 3 : returned image is 3 channel with r = g = b</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">rgb_to_grayscale</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">to_grayscale</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">num_output_channels</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">rgb_to_grayscale</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">num_output_channels</span><span class="p">)</span></div>


<div class="viewcode-block" id="erase"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.erase.html#torchvision.transforms.functional.erase">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">erase</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Erase the input Tensor Image with given value.</span>
<span class="sd">    This transform does not support PIL Image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (Tensor Image): Tensor image of size (C, H, W) to be erased</span>
<span class="sd">        i (int): i in (i,j) i.e coordinates of the upper left corner.</span>
<span class="sd">        j (int): j in (i,j) i.e coordinates of the upper left corner.</span>
<span class="sd">        h (int): Height of the erased region.</span>
<span class="sd">        w (int): Width of the erased region.</span>
<span class="sd">        v: Erasing value.</span>
<span class="sd">        inplace(bool, optional): For in-place operations. By default, is set False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor Image: Erased image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">erase</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;img should be Tensor Image. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">erase</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="gaussian_blur"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.gaussian_blur.html#torchvision.transforms.functional.gaussian_blur">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">gaussian_blur</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">sigma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Performs Gaussian blurring on the image by given kernel</span>

<span class="sd">    The convolution will be using reflection padding corresponding to the kernel size, to maintain the input shape.</span>
<span class="sd">    If the image is torch Tensor, it is expected</span>
<span class="sd">    to have [..., H, W] shape, where ... means at most one leading dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be blurred</span>
<span class="sd">        kernel_size (sequence of ints or int): Gaussian kernel size. Can be a sequence of integers</span>
<span class="sd">            like ``(kx, ky)`` or a single integer for square kernels.</span>

<span class="sd">            .. note::</span>
<span class="sd">                In torchscript mode kernel_size as single int is not supported, use a sequence of</span>
<span class="sd">                length 1: ``[ksize, ]``.</span>
<span class="sd">        sigma (sequence of floats or float, optional): Gaussian kernel standard deviation. Can be a</span>
<span class="sd">            sequence of floats like ``(sigma_x, sigma_y)`` or a single float to define the</span>
<span class="sd">            same sigma in both X/Y directions. If None, then it is computed using</span>
<span class="sd">            ``kernel_size`` as ``sigma = 0.3 * ((kernel_size - 1) * 0.5 - 1) + 0.8``.</span>
<span class="sd">            Default, None.</span>

<span class="sd">            .. note::</span>
<span class="sd">                In torchscript mode sigma as single float is</span>
<span class="sd">                not supported, use a sequence of length 1: ``[sigma, ]``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Gaussian Blurred version of the image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">gaussian_blur</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;kernel_size should be int or a sequence of integers. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;If kernel_size is a sequence its length should be 2. Got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ksize</span> <span class="ow">in</span> <span class="n">kernel_size</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ksize</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ksize</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;kernel_size should have odd and positive integers. Got </span><span class="si">{</span><span class="n">kernel_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sigma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="n">ksize</span> <span class="o">*</span> <span class="mf">0.15</span> <span class="o">+</span> <span class="mf">0.35</span> <span class="k">for</span> <span class="n">ksize</span> <span class="ow">in</span> <span class="n">kernel_size</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">sigma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sigma should be either float or sequence of floats. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">sigma</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">sigma</span><span class="p">)]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;If sigma is a sequence, its length should be 2. Got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sigma</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sigma should have positive values. Got </span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">t_img</span> <span class="o">=</span> <span class="n">img</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">_is_pil_image</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;img should be PIL Image or Tensor. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">t_img</span> <span class="o">=</span> <span class="n">pil_to_tensor</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">F_t</span><span class="o">.</span><span class="n">gaussian_blur</span><span class="p">(</span><span class="n">t_img</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">to_pil_image</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">img</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="invert"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.invert.html#torchvision.transforms.functional.invert">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">invert</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Invert the colors of an RGB/grayscale image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to have its colors inverted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            If img is PIL Image, it is expected to be in mode &quot;L&quot; or &quot;RGB&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Color inverted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">invert</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">invert</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">invert</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<div class="viewcode-block" id="posterize"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.posterize.html#torchvision.transforms.functional.posterize">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">posterize</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bits</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Posterize an image by reducing the number of bits for each color channel.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to have its colors posterized.</span>
<span class="sd">            If img is torch Tensor, it should be of type torch.uint8, and</span>
<span class="sd">            it is expected to be in [..., 1 or 3, H, W] format, where ... means</span>
<span class="sd">            it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            If img is PIL Image, it is expected to be in mode &quot;L&quot; or &quot;RGB&quot;.</span>
<span class="sd">        bits (int): The number of bits to keep for each channel (0-8).</span>
<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Posterized image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">posterize</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">bits</span> <span class="o">&lt;=</span> <span class="mi">8</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The number if bits should be between 0 and 8. Got </span><span class="si">{</span><span class="n">bits</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">posterize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">bits</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">posterize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">bits</span><span class="p">)</span></div>


<div class="viewcode-block" id="solarize"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.solarize.html#torchvision.transforms.functional.solarize">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">solarize</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Solarize an RGB/grayscale image by inverting all pixel values above a threshold.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to have its colors inverted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            If img is PIL Image, it is expected to be in mode &quot;L&quot; or &quot;RGB&quot;.</span>
<span class="sd">        threshold (float): All pixels equal or above this value are inverted.</span>
<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Solarized image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">solarize</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">solarize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">solarize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span></div>


<div class="viewcode-block" id="adjust_sharpness"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.adjust_sharpness.html#torchvision.transforms.functional.adjust_sharpness">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">adjust_sharpness</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">sharpness_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adjust the sharpness of an image.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image to be adjusted.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">        sharpness_factor (float):  How much to adjust the sharpness. Can be</span>
<span class="sd">            any non-negative number. 0 gives a blurred image, 1 gives the</span>
<span class="sd">            original image while 2 increases the sharpness by a factor of 2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: Sharpness adjusted image.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">adjust_sharpness</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">adjust_sharpness</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">sharpness_factor</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">adjust_sharpness</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">sharpness_factor</span><span class="p">)</span></div>


<div class="viewcode-block" id="autocontrast"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.autocontrast.html#torchvision.transforms.functional.autocontrast">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">autocontrast</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximize contrast of an image by remapping its</span>
<span class="sd">    pixels per channel so that the lowest becomes black and the lightest</span>
<span class="sd">    becomes white.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image on which autocontrast is applied.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            If img is PIL Image, it is expected to be in mode &quot;L&quot; or &quot;RGB&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: An image that was autocontrasted.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">autocontrast</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">autocontrast</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">autocontrast</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<div class="viewcode-block" id="equalize"><a class="viewcode-back" href="../../../generated/torchvision.transforms.functional.equalize.html#torchvision.transforms.functional.equalize">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">equalize</span><span class="p">(</span><span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Equalize the histogram of an image by applying</span>
<span class="sd">    a non-linear mapping to the input in order to create a uniform</span>
<span class="sd">    distribution of grayscale values in the output.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image on which equalize is applied.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            The tensor dtype must be ``torch.uint8`` and values are expected to be in ``[0, 255]``.</span>
<span class="sd">            If img is PIL Image, it is expected to be in mode &quot;P&quot;, &quot;L&quot; or &quot;RGB&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        PIL Image or Tensor: An image that was equalized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">equalize</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">equalize</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F_t</span><span class="o">.</span><span class="n">equalize</span><span class="p">(</span><span class="n">img</span><span class="p">)</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">elastic_transform</span><span class="p">(</span>
    <span class="n">img</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">displacement</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">interpolation</span><span class="p">:</span> <span class="n">InterpolationMode</span> <span class="o">=</span> <span class="n">InterpolationMode</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
    <span class="n">fill</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Transform a tensor image with elastic transformations.</span>
<span class="sd">    Given alpha and sigma, it will generate displacement</span>
<span class="sd">    vectors for all pixels based on random offsets. Alpha controls the strength</span>
<span class="sd">    and sigma controls the smoothness of the displacements.</span>
<span class="sd">    The displacements are added to an identity grid and the resulting grid is</span>
<span class="sd">    used to grid_sample from the image.</span>

<span class="sd">    Applications:</span>
<span class="sd">        Randomly transforms the morphology of objects in images and produces a</span>
<span class="sd">        see-through-water-like effect.</span>

<span class="sd">    Args:</span>
<span class="sd">        img (PIL Image or Tensor): Image on which elastic_transform is applied.</span>
<span class="sd">            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,</span>
<span class="sd">            where ... means it can have an arbitrary number of leading dimensions.</span>
<span class="sd">            If img is PIL Image, it is expected to be in mode &quot;P&quot;, &quot;L&quot; or &quot;RGB&quot;.</span>
<span class="sd">        displacement (Tensor): The displacement field. Expected shape is [1, H, W, 2].</span>
<span class="sd">        interpolation (InterpolationMode): Desired interpolation enum defined by</span>
<span class="sd">            :class:`torchvision.transforms.InterpolationMode`.</span>
<span class="sd">            Default is ``InterpolationMode.BILINEAR``.</span>
<span class="sd">            The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well.</span>
<span class="sd">        fill (number or str or tuple): Pixel fill value for constant fill. Default is 0.</span>
<span class="sd">            If a tuple of length 3, it is used to fill R, G, B channels respectively.</span>
<span class="sd">            This value is only used when the padding_mode is constant.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_tracing</span><span class="p">():</span>
        <span class="n">_log_api_usage_once</span><span class="p">(</span><span class="n">elastic_transform</span><span class="p">)</span>
    <span class="c1"># Backward compatibility with integer value</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">interpolation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Argument interpolation should be of type InterpolationMode instead of int. &quot;</span>
            <span class="s2">&quot;Please, use InterpolationMode enum.&quot;</span>
        <span class="p">)</span>
        <span class="n">interpolation</span> <span class="o">=</span> <span class="n">_interpolation_modes_from_int</span><span class="p">(</span><span class="n">interpolation</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">displacement</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument displacement should be a Tensor&quot;</span><span class="p">)</span>

    <span class="n">t_img</span> <span class="o">=</span> <span class="n">img</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">_is_pil_image</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;img should be PIL Image or Tensor. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">t_img</span> <span class="o">=</span> <span class="n">pil_to_tensor</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">t_img</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="o">!=</span> <span class="n">displacement</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument displacement shape should be </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">, but given </span><span class="si">{</span><span class="n">displacement</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># TODO: if image shape is [N1, N2, ..., C, H, W] and</span>
    <span class="c1"># displacement is [1, H, W, 2] we need to reshape input image</span>
    <span class="c1"># such grid_sampler takes internal code for 4D input</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">F_t</span><span class="o">.</span><span class="n">elastic_transform</span><span class="p">(</span>
        <span class="n">t_img</span><span class="p">,</span>
        <span class="n">displacement</span><span class="p">,</span>
        <span class="n">interpolation</span><span class="o">=</span><span class="n">interpolation</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
        <span class="n">fill</span><span class="o">=</span><span class="n">fill</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">to_pil_image</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">img</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-present, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>